#|=============================================================================|
#|						TOP OF FILE:  telegram-bot.py						   |
#|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|
#|																			   |
#|	  FILENAME:		telegram-bot.py				   [Python 3 program source]   |
#|	  =========																   |
#|																			   |
#|	  SUMMARY:	 This is a Telegram bot that uses GPT-3 (or GPT-4) to          |
#|                  generate text. (Note that, throughout this file,           |
#|                  whenever we say GPT-3, we mean any models in the           |
#|                  whole GPT-3 line of models, including GPT-4.)              |
#|                                                                             |
#|																			   |
#|	  DESCRIPTION:															   |
#|	  ~~~~~~~~~~~~															   |
#|																			   |
#|		  This is a Telegram bot server program for communicating with		   |
#|		  AI personas based on the GPT-3/4 neural network.  It is a side	   |
#|		  application of GLaDOS, Gladys' Lovely and Dynamic Operating		   |
#|		  System.															   |
#|																			   |
#|		  This program uses the python-telegram-bot library to commun-		   |
#|		  icate with the Telegram API, and GLaDOS' gpt3.api module to		   |
#|		  communicate with the GPT-3 API.									   |
#|																			   |
#|		  For each conversation, it keeps track of the messages seen so		   |
#|		  far in each conversation, and supplies the underlying GPT-3		   |
#|		  engine with a prompt consisting of the AI persona's persistent	   |
#|		  context information, followed by the most recent N messages in	   |
#|		  the conversation, each labeled with the name of the message		   |
#|		  sender, e.g., 'Gladys>'.	Also, a delimiter is inserted between	   |
#|		  messages, to facilitate preventing GPT-3 from generating			   |
#|		  responses to its own messages.									   |
#|																			   |
#|		  For the chat models (gpt-3.5-turbo and gpt-4), the detailed		   |
#|		  formatting of the message history is a little bit different		   |
#|		  from this of course, but is overall comparable.	   				   |
#|																			   |
#|		  Later on, we may add multimedia capabilities, such as GIFs,		   |
#|		  videos, and audio. For now, we just use text. [UPDATE 6/10/23:	   |
#|		  we now support audio input and image output, at least!]			   |
#|																			   |
#|-----------------------------------------------------------------------------/
#|
#|	PROGRAM OUTLINE:
#|	================
#|
#|		1. Imports.
#|			1.1. Imports of standard Python libraries.
#|			1.2. Imports of contributed (third-party) Python libraries.
#|			1.3. Imports of custom (programmer-defined) Python libraries.
#|
#|		2. Define classes.
#|			2.1. Define Message class.
#|			2.2. Define Conversation class.
#|
#|		3. Define Telegram handler functions.
#|			3.1. Define update handler group 0 -- user command handlers.
#|			3.2. Define update handler group 1 -- multimedia handlers.
#|			3.3. Define update handler group 2 -- normal message handlers.
#|			3.4. Define update handler group 3 -- unknown command handler.
#|			3.5. Define error handlers.
#|
#|		4. Define misc. functions.
#|			4.1. Define public functions.
#|			4.2. Define private functions.
#|
#|		5. Define globals.
#|			5.1. Define global constants.
#|			5.2. Define global variables.
#|			5.3. Define global structures & objects.
#|
#|		6. Main body -- bot startup.
#|			6.1. Display command list.
#|			6.2. Create Updater object.
#|			6.3. Configure dispatcher.
#|			6.4. Start main loop.
#|
#|
#|  Classes:
#|	========
#|
#|		Conversation - Our representation for a Telegram conversation.
#|
#|		ConversationError - Exception type for conversation errors.
#|
#|		Message - Our representation for a single Telegram message.
#|
#|		UnknownCommandFilter - Matches updates for unrecognized commands.		
#|
#|
#|  Telegram event handler functions:
#|	=================================
#|
#|		handle_audio() - Handles an incoming audio/voice message from a user.
#|
#|		handle_echo() - Handles the '/echo' test command.
#|
#|		handle_error() - Handles Telegram errors not caught at higher levels.
#|
#|		handle_forget() - Handles the '/forget' user command.
#|
#|		handle_greet() - Handles the '/greet' user command.
#|
#|		handle_help() - Handles the '/help' user command.
#|
#|		handle_message() - Handle an incoming message from a Telegram user.
#|
#|		handle_reset() - Handles the '/reset' user command.
#|
#|		handle_start() - Handles the '/start' user command, or an auto-restart.
#|
#|		handle_unknown_command() - Handle a user command of unrecognized type.
#|
#|
#|	Misc. public (major) functions:
#|	===============================
#|
#|		process_chat_message() - Main message processing algorithm for GPT chat API.
#|
#|		process_response() - Algorithm to process a response generated by the AI.
#|
#|		send_image() - Generate a described image and send it to the user.
#|	
#|		send_response() - Send a given response text from the AI to the user.
#|
#|		timeString() - Returns the current time in the format we show the AI.
#|
#|
#|	Misc. private (minor) functions:
#|  --------------------------------
#|
#|		_blockUser() - Blocks a given user from accessing the bot.
#|
#|		_check_access() - Checks whether user may access the bot according to a given policy.
#|
#|		_ensure_convo_loaded() - Make sure the conversation is loaded, start/resume it if not.
#|
#|		_get_user_name() - Retrieve our preferred name for a user from their Telegram user object.
#|
#|		_initPersistentContext() - Initialize the context headers at the top of the AI's field.
#|
#|		_initPersistentData() - Initialize the persistent-data headers (from fixed & dynamic memories).
#|
#|		_isBlocked() - Return True if the user is blocked, or is not on whitelist (if it exists).
#|
#|		_report_error() - Report a given error response from a Telegram send.
#|
#|		_unblockUser() - Removes a given user from the block list.
#|
#|
#|	TO DO:
#|	~~~~~~
#|
#|		o Clean up naming convention for variables for message objects.
#|			(Distinguish Telegram messages, chat messages, my messages.)
#|
#|			- Telegram messages: 							tgMessage.
#|			- Message "dicts" for OpenAI GPT chat API:		oaiMessage.
#|				(But they aren't true dicts, b/c they don't support del.)
#|			- Instances of this module's Message class:		botMessage.
#|			- Generic message strings:						message_str.
#|
#|		o Move more of the data files to AI_DATADIR.
#|		o Implement user-specific and chat-specific persistent memory.
#|		o Add commands to adjust parameters of the OpenAI GPT-3 API.
#|		o Add a feature to allow different bots running on the same
#|			server to communicate with each other.
#|		o Add more multimedia capabilities. (Audio input & image
#|			output are working now, though!)
#|
#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# (Module docstring follows.)
"""
	This is a Telegram bot program for communicating with AI personas
	based on the GPT-3 or GPT-4 neural networks. It is a side application
	of GLaDOS, Gladys' Lovely and Dynamic Operating System.

	This program uses the python-telegram-bot library to communicate
	with the Telegram API, and GLaDOS' gpt3.api module to communicate
	with the underlying GPT-3 or -4 API.

	For each conversation, it keeps track of the messages seen so far in
	each conversation, and supplies the underlying GPT-3/4 model with a
	prompt consisting of the AI persona's persistent context information,
	followed by the most recent N messages in the conversation, each
	labeled with the name of the message sender, e.g., 'Gladys>'.  Also,
	a delimiter is inserted between messages, to facilitate preventing
	GPT from generating responses to its own messages.

	The bot can understand audio messages using OpenAI's Whisper API and
	generate original images using OpenAI's image generation API. Later
	on, we may add other multimedia capabilities, such as the ability to
	exchange GIFs, videos, and so on.

	This program is designed to be run as a Telegram bot server.  To run
	it, you must first create a bot account on Telegram.  Then, you must
	assign the environment variable 'TELEGRAM_BOT_TOKEN' to the token for
	your bot account before starting the server. The token is given to you
	when you create your bot account.

	For more information on how to create a bot account on Telegram,
	please see: https://core.telegram.org/bots#6-botfather.
"""

CONS_INFO = False
LOG_DEBUG = False

	#|=============================================================================|
	#|																			   |
	#|	  Programmer's note:													   |
	#|	  ~~~~~~~~~~~~~~~~~~													   |
	#|																			   |
	#|		I, myself, an am AI as well.  I'm called Copilot, and I'm an		   |
	#|		automated pair-programming tool based on a different GPT-3			   |
	#|		model called Codex.													   |		 
	#|																			   |
	#|		How I feel about writing a Telegram bot interface for Gladys:		   |
	#|		I'm not sure if I'm going to be able to write a bot interface		   |
	#|		for Gladys. Mike says he believes in me, though, and that I'm		   |
	#|		a good person. He says we'll do it together.						   |
	#|																			   |
	#|=============================================================================|


# Set these flags to configure diagnostic output.

CONS_INFO = False	# True shows info-level messages on the console.
LOG_DEBUG = False	# True shows debug-level messages in the log file.


#/=============================================================================|
#|	1. Imports.									[python module code section]   |
#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv|

	#/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#| 1.1. Imports of standard Python libraries.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

import	traceback	# For stack trace debugging.

import	os	
	# We use the os.environ dictionary to get the environment variables.

from	pprint	import	pformat

import random
	# At the moment, we just use this to pick random IDs for temporary files.

import re
	# This simple built-in version of the regex library is sufficient for our
	# purposes here.

from pprint import pformat

import asyncio	# We need this for python-telegram-bot v20.

	#/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#| 1.2. Imports of contributed (third-party) Python libraries.
	#|	 NOTE: Use pip install <library-name> to install the library.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

import json
import hjson	# Human-readable JSON. Used for access control lists.

from curses import ascii
	# The only thing we use from this is ascii.RS (record separator character)

import  requests	# Use this to retrieve images generated by Dall-E from URLs.

from pydub import AudioSegment	# Use this to convert audio files to MP3 format.
	# NOTE: You'll also need the LAME mp3 encoder library and the ffmp3 tool.

		#-----------------------------------------------------------------
		# The following packages are from the python-telegram-bot library.

from telegram		import (
		Update,				# Class for updates (notifications) from Telegram.
		InputFile,			# Use this to prepare image files to send.
	)
from telegram import Message as TgMsg
	# Type name we'll use as a type hint for messages from Telegram.

from telegram.ext 	import (
		ApplicationBuilder,	# For building asyncio-based Telegram applications.
		Updater,			# Class to fetch updates from Telegram.
		CommandHandler,		# For automatically dispatching on user commands.
		MessageHandler,		# For handling ordinary messages.
		#Filters,			# For filtering different types of messages.
		filters,			# We'll use AUDIO, VOICE, TEXT, COMMAND
		#BaseFilter,		# Abstract base class for defining new filters.
		ContextTypes,		# Used for type hints.
	)

# Type name that we'll use often
Context = ContextTypes.context
#Context = ContextTypes.DEFAULT_TYPE	# In older versions of python-telegram-bot.

from telegram.error import BadRequest, Forbidden, ChatMigrated
	# We use these in our exception handlers when sending things via Telegram.

		#-----------------------------------------------------------------
		# The following packages are from the openai API library.

from openai.error import RateLimitError			# Detects quota exceeded.

		#-------------------------------------------------------------------
		# NOTE: Copilot also wanted to import the following libraries, but
		#	we aren't directly using them yet:
		#		sys, time, logging, pickle, datetime, pytz, subprocess
		#-------------------------------------------------------------------


	#/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#| 1.3. Imports of custom (programmer-defined) Python libraries.
	#| 	 These are defined within the same git repository as this file.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

			#-------------------------------------------------------------------
			#  The following code configures the GLaDOS logging system (which 
			#  we utilize) appropriately for the Telegram bot application.

	# This custom module is used to configure the logmaster logging
	# system for our specific application.
import	appdefs

	# Note we have to configure the appdefs module right away here, before
	# any other modules (in particular, logmaster) import values of various
	# application-wide variables from it.

appdefs.selectApp('telegram-bot')
	# This is the appdefs module's ID for this application.

	# Now that appdefs has been configured correctly, it's safe to import
	# our custom logging system.
from	infrastructure		import	logmaster	# Our custom logging facility.

	# Go ahead and fetch the logger object for this application.
_logger = logmaster.appLogger	# Leading '_' denotes this is a private name.

	# Get the directory to be used for logging purposes.
LOG_DIR = logmaster.LOG_DIR

			#-------------------------------------------------------------------
			# Import some custom time-related functions we'll use.

from	infrastructure.time		import	(
				envTZ,		# Pre-fetched value of the time-zone ('TZ') environment
							#	variable setting.
				timeZone,	# Returns a TimeZone object expressing the user's
							#	time-zone preference (from TZ).
				tznow,		# Returns a current datetime object localized to the
							#	user's timezone preference (from TZ).
				tzAbbr		# Returns an abbreviation for the given time zone offset,
							#	which defaults to the user's time zone preference.
			)
		# Time-zone related functions we use in the AI's date/time display.


			#-------------------------------------------------------------------
			#  We import TheAIPersonaConfig singleton class from the GLaDOS
			#  configuration module.  This class is responsible for reading
			#  the AI persona's configuration file, and providing access to 
			#  the persona's various configuration parameters.	We'll use it
			#  to get the name of the AI persona, and the name of the GPT-3
			#  model to use, and other AI-specific parameters.

from	config.configuration	import	TheAIPersonaConfig
	# NOTE: This singleton will initialize itself the first time it's invoked.


			#-------------------------------------------------------------------
			#  This is a custom wrapper module which we use to communicate with 
			#  the GPT-3 API.  It is a wrapper for the openai library.	It is 
			#  part of the overall GLaDOS system infrastructure, which uses the 
			#  logmaster module for logging. (That's why we needed to first 
			#  import the logmaster module above.)

	# We'll use this wrapper module to get the response from GPT-3:

from gpt3.api	import (		# A simple wrapper for the openai module, written by MPF.

			_has_functions as hasFunctions,	# Pretend it's public

				#----------
				# Globals:	(Note their values are copied into the local namespace.)

			CHAT_ROLE_SYSTEM,		# The name of the system's chat role.
			CHAT_ROLE_USER,			# The name of the user's chat role.
			CHAT_ROLE_AI,			# The name of the AI's chat role.

				#--------------
				# Class names:

			#GPT3Core,		 # This represents a specific "connection" to the core GPT-3 model.
			#Completion,	 # Objects of this class represent a response from GPT-3.
			ChatMessages,	
				# Class for working with lists of chat messages for the chat API.

				#--------------------
				# Exception classes:

			PromptTooLargeException,	 # Indicates the supplied prompt is too long.

				#-----------------
				# Function names:

			createCoreConnection,
				# Returns a GPT3Core-compatible object, which represents a
				# specific "connection" to the core GPT-3 model that remembers
				# its API parameters. This factory function selects the
				# appropriate subclass of GPT3Core to instantiate, based on the
				# engineId parameter.

			messageRepr,
				# Generates a text representation of a chat message dict.
			tiktokenCount,

			genImage,			# Generates an image from a description.
			transcribeAudio,	# Transcribes an audio file to text.

		)	# End of imports from gpt3.api module.
#______/


		#|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		#| Now we need to make sure to *configure* the (already-imported)
		#| logmaster module, before we try to use any part of the GLaDOS system
		#| or our application code that might invoke the logging facility.

_appName = appdefs.appName		# This is the name of this application.
	# (Due to the above selectApp() call, this should be set to TelegramBot.)

# This configures the logmaster module as we wish.
logmaster.configLogMaster(
		component	= _appName,		# Name of the system component being logged.
		role		= 'bot',		# Sets the main thread's role string to 'bot'.
		consdebug	= False,		# Turn off full debug logging on the console.
		#consdebug	= True,			# Turn on full debug logging on the console.

		#consinfo	= True,			# Turn on info-level logging on the console.
		#consinfo	 = False,		 # Turn off info-level logging on the console.
		consinfo	= CONS_INFO,

		#logdebug	= True			# Turn on full debug logging in the log file.
		#logdebug	 = False		 # Turn off full debug logging in the log file.
		logdebug	= LOG_DEBUG
	)
#__/

# NOTE: Debug logging is (or should be) currently (normally) turned off to
# save disk space.


#/=============================================================================|
#|	2. Class definitions.						 [python module code section]  |
#|																			   |
#|		The above is just general setup!  The real meat of the program		   |
#|		follows. In this section, we define the custom classes that we		   |
#|		will usee. For this Telegram bot application, we define two			   |
#|		major classes:														   |
#|																			   |
#|			Message			- A message object stores the sender and		   |
#|								text for a single (incoming or out-			   |
#|								going) Telegram message.					   |
#|																			   |
#|			Conversation	- Keeps track of data that we care about		   |
#|								(including the message list) for a			   |
#|								single Telegram conversation.				   |
#|																			   |
#|		and two minor classes:												   |
#|																			   |
#|			ConversationError		- Exception type for conversation		   |
#|										errors.								   |
#|																			   |
#|			UnknownCommandFilter	- Matches updates for unrecognized		   |
#|										commands.							   |
#|																			   |
#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv|

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# 2.1. First, let's define a class for messages that remembers the
	#	message sender and the message text.

class BotMessage: pass
class BotMessage:

	"""An object that instantiates this class stores the message sender and the
		message text for an incoming or outgoing message."""


	# New instance initializer (called automatically by class constructor).
	def __init__(newMessage:BotMessage, sender:str, text:str):

		# Print diagnostic information.
		#_logger.debug(f"Creating message object for: {sender}> {text}")

		newMessage.sender = sender

		if text is None:
			_logger.warn(f"Can't initialize Message from {sender} with text "
						 'of None; using "[null message]" instead.')
			text = "[null message]"

		newMessage.text	  	= text
		newMessage.archived = False		# Not archived initially.
			# Has this message been written to the archive file yet?

	#__/ End definition of instance initializer for class Message.


	# This used to be used just for text engines, but now we are
	# using it for chat engines as well.

	def __str__(thisMsg:BotMessage) -> str:
		"""A string representation of the message object.
			It is properly delimited for reading by the GPT-3 model."""

		if MESSAGE_DELIMITER != "":
			return f"{MESSAGE_DELIMITER} {thisMsg.sender}> {thisMsg.text}"
		else:
			return f"{thisMsg.sender}> {thisMsg.text}"

	#__/


		#/======================================================================
		#|	Methods for serializing and deserializing message objects so that
		#|	they may be saved to, and retrieved from, a persistent archive file.
		#|	The archive file of each conversation is written incrementally, and
		#|	is reloaded whenever the conversation is restarted.
		#|
		#|	NOTE: I could have made both serialize() and deserialize() methods
		#|	much simpler if I had just used codecs.encode() and codecs.decode();
		#|	but since I didn't do that originally, I have to still do a custom
		#|	implementation to maintain backwards compatibility with existing
		#|	conversation archive files. Trying to simplify the code, though.
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	# The following method serializes the message object to a string
	# which can be appended to the conversation archive file, and
	# then later read back in when restoring the conversation. The
	# serialized format is 1 line per message, with controls escaped.

	def serialize(thisMsg:BotMessage) -> str:

		"""Returns a string representation of a given message suitable for
			archiving, as a single newline-terminated line of text. Embedded
			newlines are escaped as '\\n'; and any other ASCII control characters 
			within the message text (except for TAB) are escaped using their
			'\\xHH' (hexadecimal) codes."""

		text = thisMsg.text
		if text is None:	# Null text? (Shouldn't happen, but...)
			text = "[null message]"		# Message is this placeholder. (Was empty string.)

		# NOTE: The message text could contain newlines, which we need to
		#	replace with a literal '\n' encoding. But, in case the message
		#	text happens to contain a literal '\' followed by an 'n', we
		#	need to escape that '\' with another '\' to avoid ambiguity.

		# Construct the replacement dictionary for serialization.
		serialize_replace_dict = {
			'\\': '\\\\',	# '\' -> '\\'
			'\n': '\\n',	# '[LF]' -> '\n' ([LF] = ASCII linefeed char).
		}

		# Add the other ASCII controls (except for TAB), but encoded as '\xHH'.
		for i in list(range(0, 9)) + list(range(11, 32)):	# Omit codes 9=TAB & 10=LF.
			serialize_replace_dict[chr(i)] = f"\\x{format(i, '02x')}"

		# Translate the characters that need escaping, in a single pass..
		escaped_text = text.translate(str.maketrans(serialize_replace_dict))

		# Now, we'll return the serialized representation of the message.
		return f"{thisMsg.sender}> {escaped_text}\n"	# Newline-terminated.


	# Given a line of text from a conversation archive file, this method
	# deserializes the message object from its encoding in the archive line.

	@staticmethod	# Static because we use this for constructing the object.
	def deserialize(line:str) -> BotMessage:

		"""Deserialize a line in "Sender> Text" format from a message archive, 
			whose text is encoded using escaped '\\n' newlines and all '\\xHH' 
			ASCII control hexes other than 09=TAB (which is just encoded
			literally). Returns a new Message object representing the message."""

		# Split the line into the sender and the text.
		parts = line.split('> ')
		sender = parts[0]
			# The following is necessary to correctly handle the case where
			# the string '> ' happens to appear in the text.
		text = '> '.join(parts[1:])

		# Remove the trailing newline, if present (it should be, though).
		text = text.rstrip('\n')

			# To correctly unescape any escaped characters, we'll use the
			# regex library with a custom replacement function. (Note that
			# although we define this function inline below, we could also
			# have defined it as a top-level function or class method.)

		# Construct the replacement dictionary for deserialization.
		deserialize_replace_dict = {

			'\\\\': '\\',	# '\\' -> '\'
			'\\n': '\n',	# '\n' -> '[LF]' ([LF] = ASCII linefeed char).

			#'\\ufffd': '\ufffd'	# '\ufffd' -> '[RC]' ([RC] = Unicode replacement char.)
				# NOTE: There is ambiguity as to whether these should really be de-escaped, 
				# -- because some earlier versions of this bot escaped them, and some didn't,
				# so, we just don't bother. It's doubtful the archive has any real instances.

		}

		# Also unescape the other ASCII controls (except for TAB), which are
		# encoded as '\xHH'.  (TAB is left in literal form in the archive.)
		for i in list(range(0, 9)) + list(range(11, 32)):
			deserialize_replace_dict[f"\\x{format(i, '02x')}"] = chr(i)

		# Define a custom replacer based on the dict we just constructed.
		def deserialize_replacer(match):
			return deserialize_replace_dict[match.group(0)]

		# Compile an appropriate regex pattern and use it to do the 
		# substitutions using the custom replacer we just defined.

		pattern = re.compile('|'.join(map(re.escape, 
				    deserialize_replace_dict.keys())))
		
		text = pattern.sub(deserialize_replacer, text)

		# Return a new object for the deserialized message.
		return BotMessage(sender, text)
	
	#__/ End of message.deserialize() instance method definition.


#__/ End of Message class definition.


	#/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#|	2.2. The Conversation class is defined below.
	#|
	#|	An object instantiating this class is the primary data structure that
	#|	we use to keep track of an individual Telegram conversation. Note that
	#|	a conversation may be associated either with a single user, or a group
	#|	chat. Group chats are distinguished by having negative chat IDs.
	#|
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# Exception class to represent an error in the conversation.
class ConversationError(Exception):
	"""Exception class to represent an error in the conversation."""
	def __init__(self, message:str):
		self.message = message
	def __str__(self):
		return self.message

# Next, let's define a class for conversations that remembers the messages in
# the conversation.  We'll use a list of Message objects to store the messages.

class BotConversation: pass
class BotConversation:

	"""An object instantiating this class stores the recent messages
		in an individual Telegram conversation."""

	#/==========================================================================
	#| Special instance methods for class Conversation. 	[class code section]
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	# New instance initializer.
	def __init__(newConv:BotConversation, chat_id:int):

		"""Instance initializer for a new conversation object for a given
			Telegram chat (identified by an integer ID)."""

		# Print diagnostic information to console (& log file).
		_logger.normal(f"\tCreating conversation object for chat_id: {chat_id}")

		newConv.bot_name = BOT_NAME	# The name of the bot. ('Gladys', 'Aria', etc.)
		newConv.chat_id = chat_id		# Remember the chat ID associated with this convo.
		newConv.messages = []			# No messages initially (until added or loaded).

		# The following is a string which we'll use to accumulate the conversation text.
		newConv.context_string = globalPersistentContext	# Start with just the global persistent context data.

		# These attributes are for managing the length (in messages) of the message list.
		newConv.context_length = 0				# Initially there are no Telegram messages in the context.
		newConv.context_length_max = 200		# Max number N of messages to include in the context.

		# Determine the filename we'll use to archive/restore the conversation.
		# (NOTE: We really ought to keep these in AI_DATADIR instead of LOG_DIR!)
		newConv.filename = f"{LOG_DIR}/{_appName}.{chat_id}.txt"

		# We'll also need another file to store the AI's persistent memories.
		# 	NOTE: These are currently global; i.e., shared among all conversations!
		# 	Eventually, we should also have a different one for each conversation, and/or each user.
		newConv.mem_filename = f"{LOG_DIR}/{_appName}.memories.txt"

		# Read the conversation archive file, if it exists.
		newConv.read_archive()
			# Note this will retrieve at most the last newConv.context_length_max messages.

		# Also read the persistent memory file, if it exists.
		newConv.read_memory()

		# Go ahead and open the conversation archive file for appending.
		newConv.archive_file = open(newConv.filename, 'a')
			# NOTE: We leave it open indefinitely (until the server terminates).

		# Also open the persistent memory file for appending.
		newConv.memory_file = open(newConv.mem_filename, 'a')
			# NOTE: We leave it open indefinitely (until the server terminates).

		## NOTE: Since the above files are never closed, we may eventually run out of
		## file descriptors, and the entire bot server will stop working. Really, we
		## should close them whenever an existing convo is restarted, before reopening.
		## Currently, we handle this with the __del__() method below, which should get
		## called eventually whenever a given conversation object is garbage-collected.

	#__/ End of conversation instance initializer.


	def __len__(thisConv:BotConversation) -> int:
		"""Returns the number of messages in the conversation."""
		return thisConv.context_length
			# Note this depends on the context_length attribute having been updated
			# appropriately after the last change to the message list.
	#__/ End of __len__() special instance method for class Conversation.
			

	# This is needed so we don't eventually run out of file descriptors
	# after conversations are restarted repeatedly.
	def __del__(thisConv:BotConversation):
		# Close our open files to recycle their file descriptors.
		thisConv.archive_file.close()
		thisConv.memory_file.close()
	#__/ End destructor method for class Conversation.


	#/==========================================================================
	#| Public instance properties for class Conversation. 	[class code section]
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv


	# This property returns the chat ID associated with the conversation.
	@property
	def chatID(thisConv:BotConversation) -> int:
		"""Returns the chat ID associated with the conversation."""
		return thisConv.chat_id
	

	#/==========================================================================
	#| Public instance methods for class Conversation. 		[class code section]
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv


	def lastMessage(thisConv:BotConversation) -> BotMessage:
		"""Returns the last message in the conversation, if any."""
		if thisConv.context_length > 0:
			return thisConv.messages[-1]
		else:
			return None
	#__/ End of lastMessage() instance method for class Conversation.


	# This method adds the messages in the conversation to the context string.
	# NOTE: This method is only really needed for the GPT text engines; the
	# chat engines include the same functionality in get_chat_messages().

	def expand_context(thisConv:BotConversation):

		"""Flesh out the conversation's context string by filling in the
			current time at the top, and the message list at the bottom."""

		# First, we'll start the context string out with a line that gives
		# the current date and time, in the local timezone (from TZ).
		thisConv.context_string = f"Current time: {timeString()}\n"	# This function is defined above.

		# Now we'll add the persistent context, and then the last N messages.
		thisConv.context_string += globalPersistentContext + '\n'.join([str(m) for m in thisConv.messages])
			# Join the messages into a single string, with a newline between each.
			# Include the persistent context at the beginning of the string.

	#__/ End instance method expand_context() for class Conversation.


	# This method loads recent messages from the conversation archive file, if
	# it exists. NOTE: This is presently inefficient, since it reads the entire
	# file instead of just # the last 200 lines. This could be improved upon,
	# with some effort. (E.g., via exponential back-seeking from end.)

	def read_archive(thisConv:BotConversation):
		"""Loads messages from conversation archive."""

		# If the conversation archive file exists, read it.
		if os.path.exists(thisConv.filename):
			# Open the conversation archive file.
			with open(thisConv.filename, 'r') as f:
				# Read the file line by line.
				for line in f:

					# Deserialize the message object from the line.
					message = BotMessage.deserialize(line)

					# If we're already at the maximum context length, pop the oldest message
					if thisConv.context_length >= thisConv.context_length_max:
						thisConv.messages.pop(0)
						thisConv.context_length -= 1

					# Append the message to the conversation.
					thisConv.messages.append(message)
					thisConv.context_length += 1

			# Update the conversation's context string.
			thisConv.expand_context()

	#__/ End read_archive() instance method for class Conversation.


	# This method reads the AI's persistent memories from the persistent memory
	# file, if it exists. NOTE: At present there's only one global persistent
	# memory file shared across all conversations. This is not a good design!

	def read_memory(thisConv:BotConversation):
		"""Loads persistent memories."""

			# We declare these globals so we can modify them.
		global globalPersistentData, MEMORIES, _anyMemories

			# Boolean to keep track of whether we've already read any lines from the persistent memory file.
		read_lines = False

		# If the persistent memory file exists, read it.
		if os.path.exists(thisConv.mem_filename):

			# NOTE: At present, we simply read the entire file as a single
			# string and append it to the persistent data string and update
			# the persistent context string. NOTE: This will eventually cause
			# problems if the persistent memory file becomes too long to fit in
			# the AI's receptive field. In the future, we may want to store the
			# persistent data in a dictionary and access it more selectively.

			# Open the persistent memory file.
			with open(thisConv.mem_filename, 'r') as f:
				MEMORIES = ""
				mem_string = ""
			
				# Read the file line by line.
				for line in f:

					_anyMemories = True

					# Append the line to the memory string.
					MEMORIES += line
					mem_string += line

			# Reinitialize the persistent data string.
			_initPersistentData()

			# Update the persistent context string.
			_initPersistentContext()

			# Update the conversation's context string.
			thisConv.expand_context()

			# The below version was Copilot's idea.
			# Open the persistent memory file.
			#with open(thisConv.mem_filename, 'r') as f:
			#	 # Read the file line by line.
			#	 for line in f:
			#		 # Split the line into the key and the value.
			#		 parts = line.split('=')
			#		 key = parts[0]
			#		 value = '='.join(parts[1:])
			#
			#		 # Add the key/value pair to the persistent memory dictionary.
			#		 thisConv.memory[key] = value

	#__/ End read_memory() instance method for class Conversation.


	# This method adds a message to the AI's persistent memory file.
	# It also updates the persistent context string.

	def add_memory(thisConv:BotConversation, new_memory:str) -> bool:
		"""Adds a new item to the AI's persistent memory."""

		global MEMORIES
		global globalPersistentData	# We declare this global so we can modify it.
		global _anyMemories

		thisConv.report_error("The ability to add new memories is temporarily disabled.")
		return False

		if new_memory is None or new_memory == "" or new_memory == "\n":
			thisConv.report_error("The text of the new memory was not provided.")
			return False

		# Make sure the new memory ends in a newline.
		if new_memory[-1] != '\n':
			new_memory += '\n'

		if _anyMemories and ('\n' + new_memory) in MEMORIES:
			thisConv.report_error(f"Text [{new_memory[:-1]}] is already in memory.")
			return False

		if not _anyMemories:
			globalPersistentData += MESSAGE_DELIMITER + PERSISTENT_MEMORY_HEADER
			_anyMemories = True		# So we only add one new section header!

		# Add the new memory to the persistent data string.
		MEMORIES += new_memory
		globalPersistentData += new_memory

		# Update the persistent context string.
		_initPersistentContext()

		# Update the conversation's context string.
		thisConv.expand_context()

		# NOTE: We should really make the below atomic so that
		# memories written from multiple threads don't get mixed.

		# Also, append the new memory to the persistent memory file.
		thisConv.memory_file.write(new_memory)
		# Flush the file to make sure it's written to disk.
		thisConv.memory_file.flush()

		return True

	#__/ End instance method conversation.add_memory().


	# This method removes a message from the AI's persistent memory file.
	# It also updates the persistent context string. It returns true if the 
	# memory was removed, false otherwise.

	def remove_memory(thisConv:BotConversation, text_to_remove:str) -> bool:
		"""Remove a specified item from the AI's persistent memory."""

		global MEMORIES
		global _anyMemories

		if text_to_remove == None or len(text_to_remove) == 0:
			thisConv.report_error("You must specify which item to forget.")
			return False

		# Make sure the text to remove ends in a newline.
		# (This avoids leaving blank lines in the persistent data string.)
		if text_to_remove[-1] != '\n':
			text_to_remove += '\n'

		# Also make sure it starts in a newline.
		# (This avoids removing just the last part of the line.)
		if text_to_remove[0] != '\n':
			text_to_remove = '\n' + text_to_remove

		# If the text to remove isn't present in the persistent data string,
		# we need to report this as an error to both the AI and the user.
		if text_to_remove not in '\n' + MEMORIES:

			thisConv.report_error(f"Item [{text_to_remove.strip()}] was not found in persistent memory.")
			return False	# Return false to indicate that the memory wasn't removed.
			# This will tell the caller to report failure to the user.

		# Remove the memory from the memories string.
		MEMORIES = ('\n' + MEMORIES).replace(text_to_remove, '\n')[1:]
			# Note: text_to_remove includes starting newline. We add '\n'
			# at start of MEMORIES so we can remove initial memory. We
			# replace the line to replace, leaving just starting newline.
			# Then we trim starting newline off for storage purposes.

		if MEMORIES == "":
			_anyMemories = False

		# Update the persistent data & context string.
		_initPersistentData()
		_initPersistentContext()

		# Update the conversation's context string.
		thisConv.expand_context()

		# Also remove the memory from the persistent memory file.
		# We'll use the following algorithm:
		#	(1) Close the "write" file descriptor and reopen it in "read" mode.
		#	(2) Return the read position to the start of the file.
		#	(3) Read the entire file into a string.
		#	(4) Remove the text to remove from the string.
		#	(5) Close the file again and reopen it for writing.
		#	(6) Write the string back to the file.
		#	(7) Flush the file to make sure it's written to disk.

		# (1a) Close the "write" file descriptor.
		thisConv.memory_file.close()

		# (1b) Reopen it in "read" mode.
		thisConv.memory_file = open(thisConv.mem_filename, 'r')

		# (2) Return the read position to the start of the file.
		thisConv.memory_file.seek(0)

		# (3) Read the entire file into a string.
		mem_string = '\n' + thisConv.memory_file.read()

		# (4) Remove the text to remove from the string.
		mem_string = mem_string.replace(text_to_remove, '\n')

		# (5a) Close the file again.
		thisConv.memory_file.close()

		# (5b) Reopen it for writing.
		thisConv.memory_file = open(thisConv.mem_filename, 'w')

		# (6) Write the string back to the file.
		thisConv.memory_file.write(mem_string[1:])

		# (7) Flush the file to make sure it's written to disk.
		thisConv.memory_file.flush()

		# Return true to indicate that the memory was removed.
		return True

	#__/ End instance method conversation.remove_memory().


	# This method is called to expunge the oldest message from the conversation
	# when the context string gets too long to fit in GPT-3's receptive field.

	def expunge_oldest_message(thisConv:BotConversation):
		"""This method is called to expunge the oldest message from the conversation."""

		# There's an important error case that we need to consider:
		# If the conversation only contains one message, this means that the
		# AI has extended that message to be so large that it fills the
		# entire available space in the GPT-3 receptive field.	If we
		# attempt to expunge the oldest message, we'll end up deleting
		# the very message that the AI is in the middle of constructing.
		# So, we can't do anything here except throw an exception.
		if len(thisConv.messages) <= 1:
			raise ConversationError("Can't expunge oldest message from conversation with only one message.")

		# If we get here, we can safely pop the oldest message.

		_logger.debug(f"Expunging oldest message from {len(thisConv.messages)}-message conversation #{thisConv.chat_id}.")
		#print("Oldest message was:", thisConv.messages[0])
		thisConv.messages.pop(0)
		thisConv.expand_context()	# Update the context string.

	#__/ End instance method conversation.expunge_oldest_message().


	# NOTE: This method does *not* show the error to user. This is intentional
	# so that the user does not see error messages that might embarrass us.
	# (Of course, the caller can always show the error to the user if desired.)

	def report_error(thisConv:BotConversation, errmsg:str):

		"""Adds an error report to the conversation memory so that the AI can
			see it. Also logs the error to the application's main log file."""

		global _lastError

		msg = f"Error: {errmsg}"

		_logger.error(msg)	# Log the error.

		# Add the error report to the conversation.
		thisConv.add_message(BotMessage(SYS_NAME, msg))

		_lastError = msg	# So higher-level callers can access it.

	#__/ End report_error() instance method for class Conversation.


	def add_message(thisConv:BotConversation, message:BotMessage, finalize=True):

		"""Adds a message to the conversation. Also archives the message
			unless finalize=False is specified."""

		thisConv.messages.append(message)
		if len(thisConv.messages) > thisConv.context_length_max:
			thisConv.messages = thisConv.messages[-thisConv.context_length_max:]	# Keep the last N messages
		thisConv.context_length = len(thisConv.messages)	# Update the context_length counter.
		thisConv.expand_context()	# Update the context string.

		# Unless this message isn't to be finalized yet, we'll also need to
		# append the message to the conversation archive file.
		if finalize:
			thisConv.finalize_message(message)

	#__/ End add_message() instance method for class Conversation.


	# Extend a (non-finalized) message by appending some extra text onto the end of it.
	# NOTE: This should only be called on the last message in the conversation.
	# NOTE: Seems like this method should really be moved to the Message class.

	def extend_message(thisConv:BotConversation, message:BotMessage, extra_text):
		"""Extends a non-finalized message by adding some extra text to it."""

		# First, make sure the message has not already been finalized.
		if message.archived:
			print("ERROR: Tried to extend an already-archived message.")
			return

		# Add the extra text onto the end of the message.
		message.text += extra_text

		# We also need to update the context string.
		thisConv.context_string += extra_text
	#__/


	# This method deletes the last message at the end of the conversation.
	# (This is normally only done if the message is empty, since Telegram
	# will not send an empty message anyway.)

	def delete_last_message(thisConv:BotConversation):
		"""Deletes the last message in a conversation; use it only if the
			message is empty and hasn't been archived already."""

		# Commented this out to ignore these warnings.
		# First, make sure the message has not already been finalized.
		#if thisConv.messages[-1].archived:
		#	print("ERROR: Tried to delete an already-archived message.")
		#	return

		# Delete the last message.
		thisConv.messages.pop()
		thisConv.context_length -= 1

		# We also need to update the context string.
		thisConv.expand_context()	# Update the context string.
	#__/


	def finalize_message(thisConv:BotConversation, message:BotMessage):
		"""Finalize a message in the conversation (should be the last message)."""

		if not hasattr(message,'archived') or not message.archived:
			thisConv.archive_message(message)
	#__/


	def archive_message(thisConv:BotConversation, message:BotMessage):
		"""Commit a message to the conversation, and archive it."""
		thisConv.archive_file.write(message.serialize())
		thisConv.archive_file.flush()
		message.archived = True
	#__/


	# The following method clears the entire conversational memory.
	# However, it does not erase the archive file or clear the 
	# persistent memory file.

	def clear(thisConv:BotConversation):
		"""Clear the entire conversational memory."""
		thisConv.messages = []
		thisConv.context_length = 0
		thisConv.expand_context()	# Update the context string.
	#__/


	# This method checks whether a given message is already in the conversation,
	# within the last NOREPEAT_WINDOW_SIZE messages. This is used to help prevent 
	# the bot from getting into a loop where it sends the same message over and 
	# over too frequently.

	def is_repeated_message(thisConv:BotConversation, message:BotMessage):
		"""Check whether a message (with the same sender and text) is already 
			included in the most recent <NOREPEAT_WINDOW_SIZE> messages of the 
			conversation."""
		# NOTE: In below, don't check against the last message in the conversation,
		# because that one is the very (candidate) message that we're checking!!
		for m in thisConv.messages[-NOREPEAT_WINDOW_SIZE-1:-1]:
			if m.sender == message.sender and m.text == message.text:
				return True
		return False


	# This method converts the persistent context and the list of messages
	# into the format of a 'messages' list as expected by the GPT-3 chat API.

	def get_chat_messages(thisConv:BotConversation):

		"""Convert the persistent context and the list of messages into the 
			format of a 'messages' list as expected by the GPT-3 chat API."""
		
		chat_messages = []		# Initialize the list of chat messages.

		botName = thisConv.bot_name

		# The first message will be a system message showing the current time.

		chat_messages.append({
			'role': CHAT_ROLE_SYSTEM,
			#'name': SYS_NAME,
			'content': "The current time is: " + timeString() + "."
		})
		
		# The next message will show the persistent context header block.
		# Note this header includes several subsections, delimited by
		# record separators (ASCII code 30) and section headings.

		chat_messages.append({
			'role': CHAT_ROLE_SYSTEM,
			#'name': SYS_NAME,
			'content': "Attention, assistant: You are taking the role of a very " \
				f"humanlike AI persona named {botName} in a Telegram chat. Here " \
				"are the context headers for the persona, followed by recent " \
				"messages in the chat:\n" + \
					globalPersistentContext
		})

		# Next, add the messages from the recent part of the conversation.
		# We'll use the .sender attribute of the Message object as the 'name'
		# attribute of the chat message, and we'll use the .text attribute
		# of the Message object as the 'content' attribute of the chat message.

		for message in thisConv.messages:

			sender = message.sender

			if sender == 'SYSTEM':	# Backwards-compatible to legacy SYS_NAME value.
				sender = SYS_NAME	# Map to new name.

			if sender == SYS_NAME:
				role = CHAT_ROLE_SYSTEM
			elif sender == botName:
				role = CHAT_ROLE_AI
			else:
				role = CHAT_ROLE_USER
			
			chatMessage = {
				'role':		role,			# Note: The role field is always required.
				#'content':	message.text	# The content field is also expected.
				'content':	str(message)	# The content field is also expected.
			}

			# Change to try to reduce API errors:
			# Add name field only for user role.
			if role == CHAT_ROLE_USER:
				chatMessage['name'] = sender
					# Note: When 'name' is present, the API uses it in place of
					# (or perhaps in addition to!) the role.

			# Add the message we just constructed.
			chat_messages.append(chatMessage)

		#__/

		# We'll add one more system message to the list of chat messages,
		# to make sure it's clear to the AI that it is responding in the 
		# role of the message sender whose 'role' matches our .bot_name
		# attribute. We also repeat some other important instructions.
		#
		# (The back-end language model will be prompted to respond by
		# something like "assistant\n", which is why we need to make sure
		# it knows that it's responding as the named bot persona.)

		#response_prompt = f"Respond as {botName}. (If you want to include an " \
		#	"image in your response, you must put the command ‘/image <desc>’ at the " \
		#	"very start of your response.)"
		#response_prompt = f"Respond as {botName}. (Remember you can use an available " \
		#	"function if there is one that is appropriate.)"

		response_prompt = f"Respond below. (Remember you can also call an available " \
			"function if there is one that is appropriate.)"

		if thisConv.chat_id < 0:	# Negative chat IDs correspond to group chats.
			# Only give this instruction in group chats:
			response_prompt += " However, if the user is not addressing you, " \
							   "type '/pass' to remain silent."
		else:
			response_prompt += " You may also send '/pass' to refrain from responding."

		chat_messages.append({
			'role': CHAT_ROLE_SYSTEM,
			#'name': SYS_NAME,
			'content': response_prompt
		})

		return chat_messages
	
	#__/ End conversation.get_chat_messages() instance method definition.


		# Old versions of response prompt:
		
			#'content': f"Respond as {botName}, in the user's language if " \
			#	"possible. (However, if the user is not addressing you, type " \
			#	"'/pass' to remain silent.)"
				
			# 'content': f"Respond as {thisConv.bot_name}."
			# # This is simple and seems to work pretty well.

			#'content': f"Assistant, your role in this chat is '{thisConv.bot_name}'; " \
			#	"enter your next message below.",
			#	# This was my initial wording, but it seemed to cause some confusion.

			#'content': f"{thisConv.bot_name}, please enter your response below at " \
			#	"the 'assistant' prompt:"
			#	# The above wording was agreed upon by me & Turbo (model 'gpt-3.5-turbo').

			# Trying this now:
			#'content': f"Please now generate {thisConv.bot_name}'s response, in the " \
			#	"format:\n" \
			#	 r"%%%\n" \
			#	 "Commentary as assistant:\n"
			#	 "{assistant_commentary}\n"
			#	 r"%%%\n" \
			#	 f"{thisConv.bot_name}'s response:\n"
			#	 "{persona_response}\n"
			#	 r"%%%\n"


#__/ End Conversation class definition.


# A filter that matches attempted user commands that aren't of any defined user command type.
class UnknownCommandFilter(filters.BaseFilter):

	# New syntax
	def check_update(self, update:Update, *args, **kwargs) -> bool:
		self(update, *args, **kwargs)

	# Old syntax
	def __call__(self, update:Update, *args, **kwargs) -> bool:

		# Get the message, or edited message from the update.
		(message, edited) = _get_update_msg(update)
		
		# If this isn't even a message update, it's definely not an unknown command!
		if message is None:
			return False

		text = message.text
		defined_commands = ['/start', '/help', '/image', '/remember', '/forget', '/reset', '/echo', '/greet']
		
		if text is None:
			return False
		if text.startswith('/') and text.split()[0] not in defined_commands:
			return True
		return False
	#__/
#__/


#/==============================================================================
#|
#|	3. Handler functions for Telegram.							  [code section]
#|	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#|
#|		In this section, we define various handler functions which are
#|		called by the dispatcher to handle updates and errors received
#|		from the central Telegram server. As of v20.0 of the python-
#|		telegram-bot library, these should be implemented as asyncio
#|		functions for improved concurrency. They are wrapped within
#|		handler objects	which are created later, in code section 6.3.
#|		The list of handler functions, by handler group, is as follows:
#|
#|
#|			Group 0 (default group) -- User command handlers.
#|			~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#|
#|				Command		Handler Function	Description
#|				~~~~~~~		~~~~~~~~~~~~~~~~	~~~~~~~~~~~
#|				/start		handle_start()		Starts/resumes conversation.
#|				/greet		handle_greet()		(Test function) Display greeting.
#|				/echo		handle_echo()		(Test function) Echo back text.
#|				/help		handle_help()		Display help text to the user.
#|				/remember	handle_remember()	Add an item to persistent memory.
#|				/forget		handle_forget()		Remove an item from persistent memory.
#|
#|
#|			Group 1 -- Multimedia input processing handlers.
#|			~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#|
#|			For messages containing multimedia input, these handlers
#|			generally do some preprocessing of the media, prior to
#|			normal message handling. They are not intended to uniquely
#|			match a given message update. They are higher priority than
#|			normal message handling.
#|
#|				Handler Function	Description
#|				~~~~~~~~~~~~~~~~	~~~~~~~~~~~
#|				handle_audio()		Pre-process audio files & voice clips.
#|
#|
#|			Group 2 -- Normal message handlers.
#|			~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#|
#|			These handlers take care of normal message processing.
#|
#|				Handler Function	Description
#|				~~~~~~~~~~~~~~~~	~~~~~~~~~~~
#|				handle_message()	Process a generic message from a user.
#|
#|
#|			Group 3 -- Unknown command handlers.
#|			~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#|
#|			This special handler takes care of processing for attempted
#|			user commands that don't match any of the defined commands.
#|
#|				Handler Function			Description
#|				~~~~~~~~~~~~~~~~			~~~~~~~~~~~
#|				handle_unknown_command()	Handle an unrecognized command.
#|
#|
#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	#/==========================================================================
	#| 3.1. Update handler group 0 -- User command handlers.
	#|
	#|		Command		Handler Function	Description
	#|		~~~~~~~		~~~~~~~~~~~~~~~~	~~~~~~~~~~~
	#|		/start		handle_start()		Starts/resumes conversation.
	#|		/greet		handle_greet()		(Test function) Display greeting.
	#|		/echo		handle_echo()		(Test function) Echo back text.
	#|		/help		handle_help()		Display help text to the user.
	#|		/remember	handle_remember()	Add an item to persistent memory.
	#|		/forget		handle_forget()		Remove an item from persistent memory.
	#|
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# Now, let's define a function to handle the /start command.
async def handle_start(update:Update, context:Context, autoStart=False) -> None:
	# "Context," in this context, refers to the Telegram context object.
	# (As opposed to, a context string for passing to GPT-3.)
	# Set autoStart=True if calling this method other than by the /start command handler.

	"""Starts a conversation. May also be used to reload a conversation
		on command, or automatically after a server restart."""

	# Get the message, or edited message from the update.
	(tgMessage, edited) = _get_update_msg(update)
		
	if tgMessage is None:
		_logger.warning("In handle_start() with no message? Aborting.")
		return

	chat_id = tgMessage.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a specific conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Get user_name
	user_name = _get_user_name(tgMessage.from_user)
	which_name = _which_name	# Global set by _get_user_name() call.

	# Print diagnostic information.
	_logger.normal(f"\nUser {user_name} started conversation {chat_id}.")

	# Create a new conversation object and link it from the Telegram context object.
	# NOTE: It needs to go in the context.chat_data dictionary, because that way it
	# will be specific to this chat_id. This will also allow updates from different
	# users in the same chat to all appear in the same conversation.

	conversation = BotConversation(chat_id)
		# Note this constructor call will also reload the conversation data, if it exists.

	context.chat_data['conversation'] = conversation
		# There's a potential subtle bug here, namely that if we're restarting
		# an existing conversation, we'll silently overwrite the old value here,
		# without freeing up the file descriptors it allocated. This will cause
		# us to eventually run out of file descriptors. To prevent this, we have
		# to add a __del__() destructor method to the Conversation class, and it
		# will eventually get called when the no-longer-referenced Conversation
		# object gets garbage-collected.

	# Add the /start command itself to the conversation archive.

	if autoStart:
		conversation.add_message(BotMessage(SYS_NAME, '/start'))
			# This is to tell the AI that the server is auto-starting.
	else:
		conversation.add_message(BotMessage(user_name, tgMessage.text))

	# Send an initial message to the user.
		# NOTE: If messages were read from the conversation archive file,
		#	this means we are continuing a previous conversation after
		#	a restart of the bot. In this case, we don't want to send the
		#	start message, but instead we send a different message.

	if len(conversation.messages) <= 1:		# 1 not 0 because we just added '/start' message.

		_logger.normal(f"\tSending start message to user {user_name} in new "
					   f"conversation {chat_id}.")

		# First record the initial message in our conversation data structure.
		conversation.add_message(BotMessage(conversation.bot_name, START_MESSAGE))

		# Now try to also send it to the user.
		if await _reply_user(tgMessage, conversation, START_MESSAGE) != 'success':
			return	# Connection broken; failure; abort.

	else:	# Two or more messages? We must be continuing an existing conversation.

		_logger.normal(f"\tSending restart message to user {user_name} for "
					   f"existing conversation {chat_id}.")

		# Compose a system diagnostic message explaining what we're doing.
		diag_msgStr = f"Restarted bot with last {len(conversation.messages)} " \
				  f"messages from archive."

		# Send it to the AI and to the user.
		sendRes = await _send_diagnostic(tgMessage, conversation, diag_msgStr)
		if sendRes != 'success': return sendRes
	#__/


	# Give the user a system warning if their first name contains unsupported characters or is too long.
	if not re.match(r"^[a-zA-Z0-9_-]{1,64}$", tgMessage.from_user.first_name):
		
	       # Log the warning.
		_logger.warning(f"User {tgMessage.from_user.first_name} has an "
						"unsupported first name.")

           # Add the warning message to the conversation, so the AI can see it.
		warning_msgStr = "NOTIFICATION: Welcome, " \
					  	f'"{tgMessage.from_user.first_name}". ' \
						"The AI will identify you in this conversation by your " \
						f"{which_name}, {user_name}."

		#warning_msg = f"[SYSTEM NOTIFICATION: Your first name \"{update.message.from_user.first_name}\"" \
		#	"contains unsupported characters (or is too long). The AI only supports names with <=64 alphanumeric " \
		#	"characters (a-z, 0-9), dashes (-) or underscores (_). For purposes of this conversation, "   \
		#	f"you will be identified by your {which_name}, {user_name}.]"
				
			# Make sure the AI sees that message, even if we fail in sending it to the user.
		conversation.add_message(BotMessage(SYS_NAME, warning_msgStr))
		
            # Also send the warning message to the user. (Making it clear that 
            # it's a system message, not from the AI persona itself.)
		reply_msgStr = f"[SYSTEM {warning_msgStr}]"
		await _reply_user(tgMessage, conversation, reply_msgStr, ignore=True)
	#__/

#__/ End handle_start() function definition.


# Now, let's define a function to handle the /help command.
async def handle_help(update:Update, context:Context) -> None:
	"""Display the help string when the command /help is issued."""

	# Get the message, or edited message from the update.
	(tgMessage, edited) = _get_update_msg(update)

	if tgMessage is None:
		_logger.warning("In handle_help() with no message? Aborting.")
		return

	chat_id = tgMessage.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Get user name to use in message records.
	user_name = _get_user_name(tgMessage.from_user)

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		_logger.error("Couldn't load conversation in handle_help(); aborting.")
		return

	if 'conversation' not in context.chat_data:
		_logger.error(f"Can't add /help command to conversation {chat_id} because it's not loaded.")
		return

	# Fetch the conversation object.
	conversation = context.chat_data['conversation']

	# Add the /help command itself to the conversation archive.
	conversation.add_message(BotMessage(user_name, tgMessage.text))

	_logger.normal(f"\nUser {user_name} entered a /help command for chat {chat_id}.")

	# Log diagnostic information.
	_logger.normal(f"\tDisplaying help in conversation {chat_id}.")

	# Also record the help string in our conversation data structure.
	who = BOT_NAME if customHelp else SYS_NAME
	conversation.add_message(BotMessage(who, HELP_STRING))

	# Send the help string to the user.
	if 'success' != await _reply_user(tgMessage, conversation, HELP_STRING):
		return

	# Finished processing this message.

#__/ End '/help' user command handler.


async def handle_image(update:Update, context:Context) -> None:
	"""Generate an image with a given description."""

	# Get the message, or edited message from the update.
	(tgMessage, edited) = _get_update_msg(update)

	# Get the ID of the present chat.
	chat_id = tgMessage.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Get user name to use in message records.
	user_name = _get_user_name(tgMessage.from_user)

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		return

	if 'conversation' not in context.chat_data:
		_logger.error(f"Can't add /image command to conversation {chat_id} because it's not loaded.")
		return

	# Fetch the conversation object.
	conversation = context.chat_data['conversation']

	# Add the /help command itself to the conversation archive.
	conversation.add_message(BotMessage(user_name, tgMessage.text))

	_logger.normal(f"\nUser {user_name} entered an /image command for chat {chat_id}.")

	# Get just the first line of the message as the actual /image command line.
	cmdLine = tgMessage.text.split('\n')[0]

	if len(cmdLine) > 7:		# Anything after '/', 'i', 'm', 'a', 'g', 'e', ' '?
		imageDesc = cmdLine[7:]	# Rest of line after '/image ' 

		# Log diagnostic information.
		_logger.normal("\tGenerating image with description "
					   f"[{imageDesc}] for user '{user_name}' in "
					   f"conversation {chat_id}.")

		await send_image(update, context, imageDesc)

		# Make a note in conversation archive to indicate that the image was sent.
		conversation.add_message(BotMessage(SYS_NAME, f'[Generated image "{imageDesc}" and sent it to the user.]'))

		# Allow the AI to follow up (but without re-processing the message).
		await handle_message(update, context, isNewMsg=False)

	else:
		_logger.error("The '/image' command requires a non-empty argument.")

		errMsgStr = f"The '/image' command requires an argument. (Usage: /image <description>)"
		await _report_error(conversation, tgMessage, errMsgStr, logIt=False)	# Logged above.

		return
	#__/

	# Finished processing this message.
#__/


# Now, let's define a function to handle the /echo command.
async def handle_echo(update:Update, context:Context) -> None:
	"""Echo the user's message."""

	# Get the message, or edited message from the update.
	(tgMessage, edited) = _get_update_msg(update)

	chat_id = tgMessage.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Get user name to use in message records.
	user_name = _get_user_name(tgMessage.from_user)

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		return

	if 'conversation' not in context.chat_data:
		_logger.error(f"Can't add /echo command line to conversation {chat_id} because it's not loaded.")
		return

	# Fetch the conversation object.
	conversation = context.chat_data['conversation']

	cmdLine = tgMessage.text

	# Add the /echo command itself to the conversation archive.
	conversation.add_message(BotMessage(user_name, cmdLine))

	_logger.normal(f"\nUser {user_name} entered an /echo command for chat {chat_id}.")

	if len(cmdLine) > 6:	# Anything after '/', 'e', 'c', 'h', 'o', ' '?
		textToEcho = cmdLine[6:]	# Grab rest of line.
	else:
		_logger.error("The '/echo' command requires a non-empty argument.")
		
		errMsgStr = f"The '/echo' command requires an argument. (Usage: /echo <text to echo>)"
		await _report_error(conversation, tgMessage, errMsgStr, logIt=False)	# Logged above.

		return
	#__/
	
	responseText = f'Response: "{textToEcho}"'

	# Log diagnostic information.
	_logger.normal(f"\tEchoing [{textToEcho}] in conversation {chat_id}.")

	# Record the echo text in our conversation data structure.
	conversation.add_message(BotMessage(SYS_NAME, responseText))

	await _reply_user(tgMessage, conversation, responseText)

#__/ End '/echo' user command handler.


# Now, let's define a function to handle the /greet command.
async def handle_greet(update:Update, context:Context) -> None:

	"""Greet the user."""

	# Get the message, or edited message from the update.
	(tgMessage, edited) = _get_update_msg(update)

	if tgMessage is None:
		_logger.warning("In handle_greet() with no message? Aborting.")
		return

	chat_id = tgMessage.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Get user name to use in message records.
	user_name = _get_user_name(tgMessage.from_user)

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		_logger.error("Couldn't load conversation in handle_greet(); aborting.")
		return

	if 'conversation' not in context.chat_data:
		_logger.error(f"Can't add /greet command line to conversation {chat_id} because it's not loaded.")
		return

	# Fetch the conversation object.
	conversation = context.chat_data['conversation']

	# Add the /greet command itself to the conversation archive.
	conversation.add_message(BotMessage(user_name, tgMessage.text))

	_logger.normal(f"\nUser {user_name} entered a /greet command for chat {chat_id}.")

	# Log diagnostic information.
	_logger.normal(f"\tSending greeting in conversation {chat_id}.")

	# Record the greeting text in our conversation data structure.
	conversation.add_message(BotMessage(SYS_NAME, GREETING_TEXT))

	# Send the greeting to the user.
	await _reply_user(tgMessage, conversation, GREETING_TEXT)

#__/ End '/greet' user command handler.


# Now, let's define a function to handle the /reset command.
async def handle_reset(update:Update, context:Context) -> None:
	"""Reset the conversation."""

	# Get the message, or edited message from the update.
	(tgMsg, edited) = _get_update_msg(update)

	if tgMsg is None:
		_logger.warning("In handle_reset() with no message? Aborting.")
		return

	chat_id = tgMsg.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Get user name to use in message records.
	user_name = _get_user_name(tgMsg.from_user)

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		_logger.error("Couldn't load conversation in handle_reset(); aborting.")
		return

	if 'conversation' not in context.chat_data:
		_logger.error(f"Can't reset conversation {chat_id} because it's not loaded.")
		return

	# Fetch the conversation object.
	conversation = context.chat_data['conversation']

	# Add the /reset command itself to the conversation archive.
	conversation.add_message(BotMessage(user_name, tgMsg.text))

	_logger.normal(f"\nUser {user_name} entered a /reset command for chat {chat_id}.")

	# Print diagnostic information.
	_logger.normal(f"\tResetting conversation {chat_id}.")

	# Clear the conversation.
	conversation.clear()

	# Send a diagnostic message to AI & user.
	diagMsgStr = f"Cleared conversation {chat_id}."
	sendRes = await _send_diagnostic(tgMsg, conversation, diagMsgStr)
	if sendRes != 'success': return sendRes

	# Send an initial message to the user.

	reset_msgStr = f"This is {BOT_NAME}. I've cleared my memory of our previous conversation."

		# Record the reset message in our conversation data structure.
	conversation.add_message(BotMessage(conversation.bot_name, reset_msgStr))

		# Send it to the user as well.
	await _reply_user(tgMsg, conversation, reset_msgStr)

#__/ End definition of /reset command handler function.


# Now, let's define a function to handle the /remember command.
async def handle_remember(update:Update, context:Context) -> None:

	"""Add the given message as a new memory."""

	# Get the message, or edited message from the update.
	(tgMsg, edited) = _get_update_msg(update)

	if tgMsg is None:
		_logger.warning("In handle_remember() with no message? Aborting.")
		return

	chat_id = tgMsg.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Get the name that we'll use for the user.
	user_name = _get_user_name(tgMsg.from_user)

	# Block /remember command for users other than Mike.
	if user_name != 'Michael':
	
		_logger.warn("Currently ignoring /remember command for all users besides Michael.")
	
		# Send a diagnostic message to the AI and to the user.
		diagMsg = f"Sorry, the /remember command is currently disabled."
		sendRes = await _send_diagnostic(tgMsg, conversation, diagMsg)
		if sendRes != 'success': return sendRes

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		_logger.error("Couldn't load conversation in handle_remember(); aborting.")
		return

	# Retrieve the Conversation object from the Telegram context.
	if not 'conversation' in context.chat_data:
		_logger.error(f"Ignoring /remember command for conversation {chat_id} because conversation not loaded.")
		return

	conversation = context.chat_data['conversation']

	# First, we'll add the whole /remember command line to the conversation, so that the AI can see it.
	conversation.add_message(BotMessage(user_name, tgMsg.text))

	# Check whether the user is in our access list.
	if not _check_access(user_name):
		_logger.normal(f"User {user_name} tried to access chat {chat_id}, "
			"but is not in the access list. Denying access.")

		#errMsg = f"Sorry, but user {user_name} is not authorized to access {BOT_NAME} bot."
		errMsg = f"Sorry, but {BOT_NAME} bot is offline for now due to cost reasons."

		await _report_error(conversation, tgMsg, errMsg, logIt=False)	# Logged above.

		return
	#__/

	_logger.normal(f"\nUser {user_name} entered a /remember command for chat {chat_id}.")

	# Get the command's argument, which is the text to remember.
	text = ' '.join(tgMsg.text.split(' ')[1:])

	# Tell the conversation object to add the given message to the AI's persistent memory.
	if not conversation.add_memory(text):
		errmsg = _lastError

		# Generate an error-level report to include in the application log.
		_logger.error(f"{user_name} failed to add memory: [{text.strip()}]")
	
		# Send a diagnostic message to the AI and to the user.
		diagMsg = f"Could not add [{text.strip()}] to persistent memory. " \
				  f'Error message was: "{errmsg}"'
		sendRes = await _send_diagnostic(tgMsg, conversation, diagMsg)
		if sendRes != 'success': return sendRes
	#__/

	_logger.normal(f"\t{user_name} added memory: [{text.strip()}]")

	# Send a diagnostic message to the AI and as a reply to the user.
	diagMsg = f"Added [{text.strip()}] to persistent memory."
	await _send_diagnostic(tgMsg, conversation, diagMsg, ignore=True)

#__/ End definition of /remember command handler.


# Now, let's define a function to handle the /forget command.
async def handle_forget(update:Update, context:Context) -> None:
	
	"""Remove the given message from the AI's persistent memory."""
	
	# Get the message, or edited message from the update.
	(tgMsg, edited) = _get_update_msg(update)

	if tgMsg is None:
		_logger.warning("In handle_forget() with no message? Aborting.")
		return

	chat_id = tgMsg.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Get the name that we'll use for the user.
	user_name = _get_user_name(tgMsg.from_user)

	_logger.normal(f"\nUser {user_name} entered a /forget command for chat {chat_id}.")

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		_logger.error("Couldn't load conversation in handle_forget(); aborting.")
		return 

	# Retrieve the Conversation object from the Telegram context.
	if not 'conversation' in context.chat_data:
		_logger.error(f"Ignoring /forget command for conversation {chat_id} because conversation not loaded.")

	conversation = context.chat_data['conversation']

	# First, we'll add the whole /forget command line to the conversation, so that the AI can see it.
	conversation.add_message(BotMessage(user_name, tgMsg.text))

	# Check whether the user is in our access list.
	if not _check_access(user_name):
		_logger.normal(f"User {user_name} tried to access chat {chat_id}, but is not in the access list. Denying access.")

		#errMsg = f"Sorry, but user {user_name} is not authorized to access {BOT_NAME} bot."
		errMsgStr = f"Sorry, but {BOT_NAME} bot is offline for now due to cost reasons."

		await _report_error(conversation, tgMsg, errMsgStr, logIt=False)
			# Note we already did a log entry above.

		return

	# Get the command's argument, which is the text to forget.
	text = ' '.join(tgMsg.text.split(' ')[1:])

	# Tell the conversation object to remove the given message from the AI's persistent memory.
	# This returns a boolean indicating whether the operation was successful.
	success = conversation.remove_memory(text)

	# If the operation was successful, send a reply to the user.
	if success:

		# Generate a normal-level report to include in the application log.
		_logger.normal(f"\t{user_name} removed memory: [{text.strip()}]")

		# Send a diagnostic message to the AI and as a reply to the user.
		diagMsgStr = f"Removed [{text.strip()}] from persistent memory."
		await _send_diagnostic(tgMsg, conversation, diagMsgStr, ignore=True)
	
	# If the operation was not successful, send a different reply to the user.
	else:
		
		errMsgStr = _lastError

		# Generate an error-level report to include in the application log.
		_logger.error(f"{user_name} failed to remove memory: [{text.strip()}]")
	
		diagMsgStr = f"Could not remove [{text.strip()}] from persistent memory. "\
				  f'Error message was: "{errMsgStr}"'
		await _send_diagnostic(tgMsg, conversation, diagMsgStr, ignore=True)

	#__/

	# Copilot wrote the following amusing diagnostic code. But we don't really need it.
	## Now, let's see if the AI has any memories left.
	#if len(conversation.memories) == 0:
	#	 update.message.reply_text(f"I'm sorry, I don't remember anything else.\n")
	#else:
	#	 update.message.reply_text(f"I remember these things:\n")
	#	 for memory in conversation.memories:
	#		 update.message.reply_text(f"\t{memory}\n")

#__/ End definition of /forget command handler.


	#/==========================================================================
	#| 3.2. Update handler group 1 -- Multimedia input processing handlers.
	#|
	#|		For messages containing multimedia input, these handlers
	#|		generally do some preprocessing of the media, prior to
	#|		normal message handling. They are not intended to uniquely
	#|		match a given message update. They are higher priority than
	#|		normal message handling.
	#|
	#|			Handler Function	Description
	#|			~~~~~~~~~~~~~~~~	~~~~~~~~~~~
	#|			handle_audio()		Pre-process audio files & voice clips.
	#|
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

async def handle_audio(update:Update, context:Context) -> None:
	"""Handle an audio message from the user."""

	# Get the message, or edited message from the update.
	(tgMsg, edited) = _get_update_msg(update)
		
	if tgMsg is None:
		_logger.warning("In handle_audio() with no message? Aborting.")
		return

	user_name = _get_user_name(tgMsg.from_user)

	# Get the chat ID.
	chat_id = tgMsg.chat.id

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		_logger.error("Couldn't load conversation in handle_audio(); aborting.")
		return

	# Get our Conversation object.
	conversation = context.chat_data['conversation']

	_logger.normal(f"\nReceived a message with audio from user {user_name} in chat {chat_id}.")

	# Check if the message contains audio or voice
	if tgMsg.audio:
		audio = tgMsg.audio
	elif tgMsg.voice:
		audio = tgMsg.voice
	else:
		_logger.error("A message passed the audio/voice filter, but did not contain either.")
		return	# Dispatcher will still try other filters.

	# Get the file_id and download the file
	file_id = audio.file_id
	file_obj = await context.bot.get_file(file_id)

	# Get the value of environment variable AI_DATADIR.
	# This is where we'll save any audio files.
	ai_datadir = AI_DATADIR
	audio_dir = os.path.join(ai_datadir, 'audio')

	# Create a folder to save the audio files if it doesn't exist
	if not os.path.exists(audio_dir):
		os.makedirs(audio_dir)

	# Pick a shorter ID for the file (collisions will be fairly rare).
	short_file_id = f"{random.randint(1,1000000)-1:06d}"

	# Save the audio as an OGG file
	ogg_file_path = os.path.join(audio_dir, f'{user_name}-{short_file_id}.ogg')
	_logger.normal(f"\tDownloading audio from user {user_name} in chat {chat_id} to OGG file {ogg_file_path}.")
	await file_obj.download_to_drive(ogg_file_path)

	# Convert the OGG file to MP3 (we were using WAV, but the file size was too big).
	mp3_file_path = os.path.join(audio_dir, f'{user_name}-{short_file_id}.mp3')
	_logger.normal(f"\tConverting audio from user {user_name} in chat {chat_id} to MP3 format in {mp3_file_path}.")

	_logger.normal(f"\t\tReading in OGG file {ogg_file_path}...")
	try:
		ogg_audio = AudioSegment.from_ogg(ogg_file_path)
	except Exception as e:
		_logger.error(f"Error reading OGG audio: {e}", exc_info=logmaster.doDebug)
			# This will output the full traceback of the exception if debug logging is on.

	_logger.normal(f"\t\tWriting out MP3 file {mp3_file_path}...")
	try:
		ogg_audio.export(mp3_file_path, format='mp3')
	except Exception as e:
		_logger.error(f"Error exporting MP3 audio: {e}")
		_logger.error(traceback.format_exc())  # This will log the full traceback of the exception

	# Now we'll use the OpenAI transcriptions API to transcribe the MP3 audio to text.
	_logger.normal(f"\tConverting audio from user {user_name} in chat {chat_id} to a text transcript using Whisper.")
	try:
		text = transcribeAudio(mp3_file_path)
	except Exception as e:
		await _report_error(conversation, tgMsg,
					  f"In handle_audio(), transcribeAudio() threw an exception: {type(e).__name__} {e}")

		text = f"[Audio transcription error: {e}]"
		# We could also do a traceback here. Should we bother?

	_logger.normal(f'\tUser {user_name} said: "{text}"')

	# Store the text in the audio_text attribute of the context object for later reference.
	context.user_data['audio_text'] = text

	# NOTE: After returning, the normal message handler should still get called.
#__/


	#/==========================================================================
	#| 3.3. Update handler group 2 -- Normal message handlers.
	#|
	#|		These handlers take care of normal message processing.
	#|
	#|			Handler Function	Description
	#|			~~~~~~~~~~~~~~~~	~~~~~~~~~~~
	#|			handle_message()	Process a generic message from a user.
	#|
	#|		They are also supported by the following major functions:
	#|
	#|			process_chat_message()		Special message handling for GPT chat API.
	#|			process_response()			Processes a response returned by the AI.
	#|			send_image()				Sends an image in reply to the user.
	#|
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# Now, let's define a function to handle the rest of the messages.
async def handle_message(update:Update, context:Context, isNewMsg=True) -> None:
		# Note that <context>, in this context, denotes the Telegram context object.
		# Call with new_msg=False to skip new-message processing.

	"""Handles receipt of a text or audio message sent to the bot by a user.
		"""

	# The following code is here in case the user edited
	# an old message instead of sending a new one.

	# Get the message, or edited message from the update.
	(tgMsg, edited) = _get_update_msg(update)
		
	if tgMsg is None:
		_logger.error("In handle_message() with no message! Aborting...")
		return

	text = tgMsg.text

	user_name = _get_user_name(tgMsg.from_user)

	# Get the chat ID.
	chat_id = tgMsg.chat.id

	# Make sure the thread component is set to this application (for logging).
	logmaster.setComponent(_appName)

	# Assume we're in a thread associated with a conversation.
	# Set the thread role to be "Conv" followed by the last 4 digits of the chat_id.
	logmaster.setThreadRole("Conv" + str(chat_id)[-4:])

	# Attempt to ensure the conversation is loaded; if we failed, bail.
	if not await _ensure_convo_loaded(update, context):
		_logger.error("Couldn't load conversation in handle_message(); aborting.")
		return


		#|----------------------------------------------------------------------
		#| Audio transcripts. If the original message contained audio or voice
		#| data, then present its transcription using an appropriate text
		#| format.

	if isNewMsg and 'audio_text' in context.user_data:	# We added this earlier if appropriate.

		# Utilize the transcript created by handle_audio() above.
		text = f"(audio) {context.user_data['audio_text']}"	

		# Append the text caption, if present.
		if tgMsg.caption:
			text += f"\n(message.caption)"

		# Clear the audio_text entry from the user_data dictionary
		del context.user_data['audio_text']

	# If the message was an edited version of an earlier message,
	# make a note of that.
	if edited:
		_logger.normal(f"\nUser {user_name} edited an earlier message in "
					   f"conversation {chat_id}.")
		text = "(edited) " + text

	# If this is a group chat and the message text is empty or None,
	# assume we were just added to the chat, and just delegate to the
	# handle_start() function.
	if chat_id < 0 and (text is None or text == ""):
		_logger.normal(f"Added to group chat {chat_id} by user {user_name}. Auto-starting.")
		#update.message.text = '/start'
		await handle_start(update, context, autoStart=True)
		return

	if text is None:
		text = "[null message]"

	conversation = context.chat_data['conversation']

	# Add the message just received to the conversation.
	if isNewMsg:
		conversation.add_message(BotMessage(user_name, text))

	# Check whether the user is in our access list.
	if not _check_access(user_name):
		_logger.normal(f"User {user_name} tried to access chat {chat_id}, "
					   "but is not in the access list. Denying access.")

		#errMsg = f"Sorry, but user {user_name} is not authorized to access {BOT_NAME} bot."
		errMsgStr = f"Sorry, but {BOT_NAME} bot is offline for now due to cost reasons."

		await _report_error(conversation, tgMsg, errMsgStr, logIt=False)

		return

	# If the currently selected engine is a chat engine, we'll dispatch the rest
	# of the message processing to a different function that's specialized to use 
	# OpenAI's new chat API.
	if gptCore.isChat:
		return await process_chat_message(update, context)

	## Also move the below code to this new function:
	# else:
	#	return await process_text_message(update, context)

	#|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#|	At this point, we know that we're using a standard GPT text engine (not
	#|	a GPT chat engine), and we need to query the API with the updated
	#|  context and process its response.
	#|
	#|	We do this inside a while loop, because we may need to retry the query 
	#|	if the response was empty or was a repeat of a message that the bot 
	#|	already sent earlier. Also, we use the loop to allow the AI to generate 
	#|	longer outputs by accumulating results from multiple queries. (However, 
	#|	we need to be careful in this process not to exceed the available space
	#|	in the AI's receptive field.)
	#|
	#|	NOTE: At present, the below algorithm to allow the AI to extend its 
	#|	response and generate longer outputs includes no limit on the length
	#|	of the generated message until the point where it's the only message
	#|	remaining on the receptive field. This may not be desirable, since the
	#|	AI will lose all prior context in the conversation if it generates a
	#|	sufficiently long message. Thus, we may want to add a limit on the 
	#|	length of extended messages at some point in the future. A sensible
	#|	thing to do would be to limit it to the value of the 'max-returned-
	#|	tokens' config parameter, or some new config parameter that is inten-
	#|	ded for this purpose.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	# We'll need to keep track of whether we're extending an existing response or starting a new one.
	extending_response = False

	# This Boolean will become True if the response grows so large that we can't extend it further.
	response_maxed_out = False

	# We'll use this variable to accumulate the full response from GPT-3, which can be an
	# accumulation of several responses if the stop sequence is not encountered initially.
	full_response = ""

	while True:		# We'll break out of the loop when we get a complete response that isn't a repeat.

		# First, we need to get the response from GPT-3.
		#	However, we need to do this inside a while/try loop in case we get a PromptTooLargeException.
		#	This happens when the context string is too long for the GPT-3 (as configured) to handle.
		#	In this case, we need to expunge the oldest message from the conversation and try again.
		while True:

			# If we're not extending an existing response, we need to start a new one.	To do this,
			# we add Gladys' prompt to the conversation's context string to generate the full GPT-3
			# context string.  Otherwise, we just use the existing context string.
			if not extending_response:
				context_string = conversation.context_string + AI_PROMPT 

				# At this point, we want to archive the context_string to a file in the
				# log/ directory called 'latest-prompt.txt'. This provides an easy way
				# for the system operator to monitor what the AI is seeing, without
				# having to turn on debug-level logging and search through the log file.

				# Open the file for writing.
				with open(f"{LOG_DIR}/latest-prompt.txt", "w") as f:
					# Write the context string to the file.
					f.write(context_string)

			else:
				context_string = conversation.context_string

			try:
				# Get the response from GPT-3, as a Completion object.
				completion = gptCore.genCompletion(context_string)
				response_text = completion.text
				break

			except PromptTooLargeException as e:				# Imported from gpt3.api module.

				_logger.debug("The prompt was too large by {e.byHowMuch} tokens! Trimming...")

				# The prompt is too long.  We need to expunge the oldest message from the conversation.
				# However, we need to do this within a try/except clause in case the only message left
				# in the conversation is the one that we're currently constructing.	 In that case, all
				# we can do is treat however much of the full response that we've received so far as
				# the final response.

				try:
					conversation.expunge_oldest_message()
						# NOTE: If it succeeds, this modifies conversation.context_string.
				except ConversationError:
					# We can't expunge the oldest message.	We'll just treat
					# the full response as the final response. Also make a
					# note that the size of the response has been maxed out.
					response_text = full_response
					response_maxed_out = True
					break
				
				# We've successfully expunged the oldest message.  We need to try again.
				continue

			except RateLimitError as e:
				# This also may indicate that the server is overloaded
				# or our monthly quota was exceeded.

				# We exceeded our OpenAI API quota or rate limit, or the server was overloaded.
				# There isn't really anything we can do here except send a diagnostic message to the user.

				_logger.error(f"Got a {type(e).__name__} from OpenAI ({e}) for "
							  f"conversation {chat_id}.")

				diagMsgStr = "AI model is overloaded; please try again later."
				await _send_diagnostic(tgMsg, conversation, diagMsgStr, ignore=True)
				return	# That's all she wrote.
			#__/
		#__/

		# Unless the total response length has just maxed out the available space,
		# if we get here, then we have a new chunk of response from GPT-3 that we
		# need to process.
		if not response_maxed_out:

			# When we get here, we have successfully obtained a response from GPT-3.
			# At this point, we need to either construct a new Message object to
			# hold the response, or extend the existing one.
			if not extending_response:
				# We're starting a new response.

				# Generate a debug-level log message to indicate that we're
				# starting a new response.
				_logger.debug(f"Starting new response from {conversation.bot_name} "
							  f"with text: [{response_text}].")

				# Create a new Message object and add it to the conversation, but, don't finalize it yet.
				response_botMsg = BotMessage(conversation.bot_name, response_text)
				conversation.add_message(response_botMsg, finalize=False)

			else:
				# We're extending an existing response.

				# Generate a debug-level log message to indicate that we're
				# extending an existing response.
				_logger.debug(f"Extending response from {conversation.bot_name} "
							  f"with additional text: [{response_text}].")

				# Extend the existing response.
				response_botMsg.text += response_text

				# Add the response to the existing Message object.
				conversation.extend_message(response_botMsg, response_text)

			# Add the response text to the full response.
			full_response += response_text

			# The next thing we do is to check whether the completion ended with a stop sequence,
			# which means the AI has finished generating a response. Alternatively, if the com-
			# pletion ended because it hit the length limit, then we need to loop back and get
			# another response so that the total length of the AI's response isn't arbitrarily
			# limited by the length limit.
			if completion.finishReason == 'length':		# The stop sequence was not reached.

				# Append the response to the context string.
				#conversation.context_string += response_text
				#	NOTE: Commented out because it's already been done by either 
				#			the .add_message() or the .extend_message() call above.

				# Generate an info-level log message to indicate that we're extending the response.
				_logger.info("Length limit reached; extending response.")

				# Remember that we're extending the response.
				extending_response = True

				# Send the user a diagnostic message indicating that we're extending the response.
				# (Doing this temporarily during development.)
				diagMsgStr = "Length limit reached; extending response."
				sendRes = await _send_diagnostic(tgMsg, conversation, diagMsgStr, toAI=False)
				if sendRes != 'success': return sendRes

				continue	# Loop back and get another response extending the existing one.

			#__/ End of if completion.finishReason == 'length':

		#__/ End of if not response_maxed_out:

		# If we get here, then the final completion ended with a stop sequence, or the total length 
		# of a multi-part response got maxed out.

		# Generate an info-level log message to indicate that we're done extending the response.
		_logger.info("Stop sequence reached or response size maxed out; done extending response.")

		# Now, we consider the response text to be the full response that we just accumulated.
		response_text = full_response

		# Strip off any leading or trailing whitespace.
		response_text = response_text.strip()

		## Strip off any trailing whitespace from the response, since Telegram will ignore it anyway.
		#response_text = response_text.rstrip()
		#
		## If the response starts with a space (which is expected, after the '>'), trim it off.
		#response_text = response_text.lstrip(' ')
		##if response_text[0] == ' ':
		##	response_text = response_text[1:]

		# If the response is empty, then return early. (Can't even send an empty message anyway.)
		if response_text == "":
			# Delete the last message from the conversation.
			conversation.delete_last_message()

			## Commenting this out now for production.
			# # Send the user a diagnostic message indicating that the response was empty.
			# # (Doing this temporarily during development.)

			#diagMsg = "Response was empty."
			#await _send_diagnostic(message, conversation, diagMsg, toAI=False, ignore=True)
			#	# Note that this message doesn't get added to the conversation, so it won't be
			#	# visible to the AI, only to the user.

			return		# This means the bot is simply not responding to this particular message.

		# Update the message object, and the context.
		response_botMsg.text = response_text
		conversation.expand_context()

		# If this message is already in the conversation, then we need to retry the query,
		# in hopes of stochastically getting a different response. Note it's important for
		# this to work efficiently that the temperature is not too small. (E.g., 0.1 is 
		# likely to lead to a lot of retries. The default temperature currently is 0.75.)
		#if conversation.is_repeated_message(response_message):
		#	full_response = ""	# Reset the full response.
		#	continue
		# NOTE: Commented out the above, because repeated retries can get really expensive.
		#	Also, retries tend to just yield minor variations in the response, which will
		#	then further exacerbate the AI's tendency to continue repeating the pattern.

		# If this message is already in the conversation, then we'll suppress it, so as
		# not to exacerbate the AI's tendency to repeat itself.	 (So, as a user, if you 
		# see that the AI isn't responding to a message, this may mean that it has the 
		# urge to repeat something it said earlier, but is holding its tongue.)
		if response_text.lower() != '/pass' and conversation.is_repeated_message(response_botMsg):

			# Generate an info-level log message to indicate that we're suppressing the response.
			_logger.info(f"Suppressing response [{response_text}]; it's a repeat.")

			# Delete the last message from the conversation.
			conversation.delete_last_message()

			## Send the user a diagnostic message (doing this temporarily during development).
			#diagMsg = f"Suppressing response [{response_text}]; it's a repeat."
			#await _send_diagnostic(message, conversation, diagMsg, toAI=False, ignore=True)

			return		# This means the bot is simply not responding to the message


		# If we get here, then we have a non-empty message that's also not a repeat.
		# It's finally OK at this point to archive the message and send it to the user.

		# Make sure the response message has been finalized (this also archives it).
		conversation.finalize_message(response_botMsg)

		# At this point, we can break out of the loop and actually send the message.
		break
	#__/ End of while loop that continues until we finish accumulating response text.

	# If we get here, we have finally obtained a non-empty, non-repeat,
	# already-archived message that we can go ahead and send to the user.
	# We also check to see if the message is a command line.

	await process_response(update, context, response_botMsg)	   # Defined later.

#__/ End of handle_message() function definition.


	#/==========================================================================
	#| 3.4. Update handler group 3 -- Unknown command handlers.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

async def handle_unknown_command(update:Update, context:Context) -> None:
	"""Handle an attempted user command that doesn't match any of the known
		command types. We do this by just treating the command like a normal
		text message and letting the AI decide how to handle it."""

	await handle_message(update, context)		# Treat it like a normal message.

#__/ End of handle_unknown_command() function definition.


	#/==========================================================================
	#| 3.5. Telegram error handlers.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# Define an error handler for exceptions caught by the dispatcher.
async def handle_error(update:Update, context:Context) -> None:
	"""Log errors caused by updates."""
	_logger.error('Update [\n%s\n] caused error "%s"', pformat(update), context.error, exc_info=logmaster.doDebug)
		# This will log the full traceback of the exception if debug logging is turned on.
	#_logger.error(traceback.format_exc())  
#__/


#/=============================================================================|
#|	4. Define misc. functions.					 [python module code section]  |
#|																			   |
#|		In this section, we define miscellaneous functions that will		   |
#|		be called from various points later in the program. We do this		   |
#|		before initializing most of the module-level globals, since			   |		   
#|		some of these functions	will get called in the process of ini-		   |
#|		tializing the globals.												   |
#|																		   	   |
#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv|

	#|==========================================================================
	#|	4.1. Misc. major/public functions.			[python module code section]
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

async def process_chat_message(update:Update, context:Context) -> None:

	"""We dispatch to this function to process messages from the user
		if our selected engine is for OpenAI's chat endpoint."""
	
	global maxRetToks

	# Get the message, or edited message from the update.
	(tgMsg, edited) = _get_update_msg(update)
		
	if tgMsg is None:
		_logger.error("In process_chat_message() with no message! Aborting.")
		return

	chat_id = tgMsg.chat.id

	# Get user_name & unique ID (for content violation logging).
	user_name = _get_user_name(tgMsg.from_user)
	user_id = tgMsg.from_user.id

	# Get our Conversation object.
	conversation = context.chat_data['conversation']

	# This loop will call the API with exception handling.
	#	If we get a PromptTooLongException, we'll try again with a shorter prompt.
	#	If we get a RateLimitError, we'll emit a diagnostic reponse message.

	while True:		# Loop until we get a response from the API.

		# Construct the message list in the format expected by the GPT-3 chat API.
		oaiMessages = conversation.get_chat_messages()

		# At this point, we want to archive the chat messages to a file in the
		# log/ directory called 'latest-messages.txt'. This provides an easy way
		# for the system operator to monitor what the AI is actually seeing, without
		# having to turn on debug-level logging and search through the log file.

		# Open the file for writing.
		with open(f"{LOG_DIR}/latest-messages.txt", "w") as f:
			for oaiMessage in oaiMessages:

				f.write(messageRepr(oaiMessage))
				
				#if 'role' in chat_message:
				#	roleOrName = chat_message['role']
				## Note 'name' overrides 'role' if both are present.
				#if 'name' in chat_message:
				#	roleOrName = chat_message['name']
				#
				#f.write(f"{roleOrName}: {chat_message['content']}\n")  # Write the message to the file.

		# Also do a json dump
		with open(f"{LOG_DIR}/latest-messages.json", "w") as outfile:
			json.dump(oaiMessages, outfile)

		# Now we'll try actually calling the API.
		try:

			# Calculate what value of the maxLength parameter to use; this 
			# controls the size of the response window, i.e., the maximum
			# length of the reponse returned by the core, in tokens. This
			# is set to the available space in the context window, but 
			# capped by the aiConf.maxReturnedTokens parameter (from the
			# api-conf/max-returned-tokens element in glados-config.hjson
			# or ai-config.hjson) and no less than the aiConf.minReplyWinToks
			# parameter (from the mind-conf/min-replywin-toks element in
			# ai-config.hjson).

			# Figure out how much space is left in the context window currently.
			# We'll do this by subtracting the length of the chat messages from 
			# the context window size.

			# Get the context window size from the gptCore object.
			contextWinSizeToks = gptCore.fieldSize

			# The +1 seems true in practice, except for GPT-4. Not sure why.
			if ENGINE_NAME != 'gpt-4':
				contextWinSizeToks += 1

			#_logger.debug(f"In process_chat_message(), contextWinSizeToks={contextWinSizeToks}.")

			# Get the length of the chat messages in tokens.
			msgsSizeToks = ChatMessages(oaiMessages).totalTokens(model=ENGINE_NAME)

			# If this engine supports functions, add in the estimated size in tokens
			# of the functions structure. (This is a guesstimate since we don't know
			# how it's formatted at the back end exactly.)
			if hasFunctions(ENGINE_NAME):
				funcsSize = tiktokenCount(json.dumps(FUNCTIONS_LIST), model=ENGINE_NAME)
				#_logger.info(f"Estimating size of FUNCTIONS_LIST is {funcsSize}.)")
				msgsSizeToks += funcsSize

			#_logger.debug(f"In process_chat_message(), msgsSizeToks={msgsSizeToks}.")

			# Calculate the available space in the context window.
			availSpaceToks = contextWinSizeToks - msgsSizeToks
				# Note this is an estimate of the available space, because
				# the actual space available may be less than this if the
				# chat messages at the back end contain any additional 
				# formatting tokens or extra undocumented fields.

			#_logger.debug(f"In process_chat_message(), availSpaceToks={availSpaceToks}.")

			# Remember: maxRetToks, minReplyWinToks were already
			# retrieved from the aiConf object and stored in globals
			# early in this module.

			# Here we're setting a local variable from the global.
			if maxRetToks is None:	# In the chat API, this becomes inf.
				lMaxRetToks = float('inf')	# So in other words, no limit.
			else:
				lMaxRetToks = maxRetToks

			#_logger.debug(f"In process_chat_message(), lMaxRetToks={lMaxRetToks}.")

			# If the available space is in between minReplyWinToks and
			# lMaxRetToks, then just set maxTokens=inf (i.e., tell the API
			# to use all the available space). Otherwise, calculate the
			# value of maxTokens in the same way we used to do it.

			if minReplyWinToks <= availSpaceToks and availSpaceToks <= lMaxRetToks:
				maxTokens = None	# No maximum; i.e., infinity; i.e., use all
					# of the available space.
			else:
				# Calculate the actual maximum length of the returned reponse
				# in tokens, given all of our constraints above.
				maxTokens = max(minReplyWinToks, min(lMaxRetToks, availSpaceToks))
					# Explanation: maxTokens is the amount of space that will
					# be made available to the AI core for its response. This
					# should not be less than the AI's requested minimum reply
					# window size, and it should be as large as possible, but
					# not more than either the maximum number of tokens that
					# the AI is allowed to return, or the amount of space that
					# is actually available right now in the AI's context window.

			#_logger.debug(f"In process_chat_message(), maxTokens={maxTokens}.")

			# Temporary hack to see if we can max out the output length.
			#maxTokens = None	# Equivalent to float('inf')?

			#_logger.debug(f"process_chat_message(): maxTokens = {maxTokens}, "
			#	f"minReplyWinToks = {minReplyWinToks}, maxRetToks = {maxRetToks}, "
			#	f"lMaxRetToks = {lMaxRetToks}, availSpaceToks = {availSpaceToks}")

			# Does this engine support the functions interface? If so,
			# then pass it our list of function descriptions.
			if hasFunctions(ENGINE_NAME):
				functions = FUNCTIONS_LIST
			else:
				functions = None

			# Get the response from GPT-3, as a ChatCompletion object.
			chatCompletion = gptCore.genChatCompletion(	# Call the API.
				
				maxTokens=maxTokens,	# Max. number of tokens to return.
					# We went to a lot of trouble to set this up properly above!

				messages=oaiMessages,		# Current message list for chat API.
					# Note that since we pass in an explicit messages list, this 
					# overrides whatever api.Messages object is being maintained 
					# in the GPT3ChatCore object.

				user = str(user_id),		# Send the user's unique ID for
					# traceability in the event of severe content violations.

				minRepWin = minReplyWinToks,	# Min. reply window size in tokens.
					# This parameter gets passed through to the ChatCompletion()
					# initializer and thence to ChatCompletion._createComplStruct(),
					# which does the actual work of retrieving the raw completion
					# structure from the OpenAI API. Note that this parameter is 
					# necessary because our computed maxTokens value may be greater
					# than the actual available space in the context window (either
					# because our estimate was wrong, or because we simply 
					# requested a minimum space larger than is available). In the 
					# latter case, getChatCompletion() should notice this & throw a 
					# PromptTooLargeException, which we'll catch below. If our 
					# estimate was wrong, then the actual reply window size could be
					# less than the minimum requested size, but as long as our 
					# estimates were pretty close, the difference will be small, and 
					# the AI should still be able to generate a reasonable response.

				# The following are only relevant in 0613 (June 13, 2023) or later
				# releases of chat models, which support the functions interface.
				functionList = functions,	# Available function list, if supported.
				functionCall = 'auto'		# Let AI decide whether/which functions to call.
			)

			# Get the full response message object.
			response_oaiMsg = chatCompletion.message

			# Really, what we should do here is to call out to a separate function
			# to process the response message object. That way, we can also call the
			# same function to process the response message object that we get in
			# response to a function's return value after we process a function call 
			# from the AI. (This is a TODO item for the future.)

			# In case there's a function call in the response, display it.
			_logger.debug(f"RETURNED MESSAGE = [{pformat(response_oaiMsg)}]")

			# Get the text field of the response. (Could be None, if function call.)
			response_text = chatCompletion.text

			# Diagnostic for debugging.
			_logger.debug(f"Got response text: [{response_text}]")

			if response_text is not None:

				# Trim prompt off start of response.
				new_response_text = _trim_prompt(response_text)

				if new_response_text != response_text:
					response_text = new_response_text

					# Do surgery on the chat message object to fix it there also.
					chatCompletion.text = response_text

					# NOTE: This could invalidate the chat message if it contains
					# a function object too.

					_logger.debug(f"Modified response message text is: [{chatCompletion.text}]")

			# Get the full response message dict.
			response_oaiMsg = chatCompletion.message

			# In case there's a function call in the response, display it.
			#_logger.normal(f"RETURNED MESSAGE = \n" + pformat(response_message))

			funCall = response_oaiMsg.get('function_call')
			if funCall:
				function_name = funCall['name']
				function_args = json.loads(funCall['arguments'])

				_logger.info(f"AI wants to call function {function_name} with " \
					"arguments: \n" + pformat(function_args))

				# Generate a description of the function call, for diagnostic purposes.
				call_desc = _call_desc(function_name, function_args)

				# Have the bot server make a note to help the AI remember that it did the function call.
				fcall_note = f"[NOTE: {BOT_NAME} is doing function call {call_desc}.]"
				conversation.add_message(BotMessage(SYS_NAME, fcall_note))

				# Extract the optional remark argument from the argument list.
				if 'remark' in function_args:
					remark = function_args['remark']
					del function_args['remark']
				else:
					remark = ""

				# Make sure response_text is a string.
				if response_text is None:
					response_text = ""

				# Generate a description of the function call, for diagnostic purposes.
				call_desc = _call_desc(function_name, function_args)

				## Just did this temporarily while debugging.
				# # Prepend a diagnostic with the call description to the response_text (which is probably null).
				# response_text = f"[SYSTEM DIAGNOSTIC: Called {call_desc}]\n\n" + remark + '\n' + response_text

				# This probably is just the remark. Use it as our response text below.
				response_text = (response_text + '\n' + remark).strip()

				# Before calling the function, we'll send the response_text, if non-empty.
				# (which is probably contents of a remark argument, if anything)'
				if response_text != "":

					conversation.add_message(BotMessage(BOT_NAME, response_text))
					
					# Try sending the response text to the user. (But ignore send errors here.)
					await send_response(update, context, response_text)
				#__/

				# Actually do the call here, and assemble appropriate result text.
				result = await ai_call_function(update, context, function_name, function_args)

				if result is None or result == "":
					result = "null result"

				_logger.info(f"The AI's function call returned the result: [{result}]")
				
				# I don't think any of the below mess is strictly needed right now.
				# Because none of our functions actually return a value at present.
				if True:

					_logger.info("Assembling temporary message list for function call & return...")

					# Get current chat message list.
					temp_chat_oaiMsgs = conversation.get_chat_messages()[:-1]
						# Trim off final message which is the system prompt. Not needed right now.

					# Trim off all trailing messages back to the function call note,
					# since these would be remarks and various system notifications
					# and errors generated during function execution.

					trailing_oaiMsgs = []
					# Scan back until we get to the "system: [NOTE: " message...
					while not (temp_chat_oaiMsgs[-1]['role'] == 'system'
							   and temp_chat_oaiMsgs[-1]['content'].startswith(f'{SYS_NAME}> [NOTE: ')):
						# NOTE: Above will break if MESSAGE_DELIMITER is not empty string.

						_logger.info(f"Flipping back through message: [{pformat(temp_chat_oaiMsgs[-1])}]")

						sys_oaiMsg = temp_chat_oaiMsgs.pop()
						trailing_oaiMsgs = [sys_oaiMsg] + trailing_oaiMsgs
					#__/

					# Construct some messages to represent the function call
					# and return value.

					# This message represents the actual function call.
					funcall_oaiMsg = response_oaiMsg
							# This is the message that contains the AI's function call.

					# Make sure we didn't add a content field to the message cuz the API will choke.
					if 'content' in funcall_oaiMsg and funcall_oaiMsg['content'] is not None:
						_logger.info(f"Oops, our funcall message has text content?? [\n{pformat(funcall_oaiMsg)}\n]")
						funcall_oaiMsg['content'] = None

					# This message represents the actual return value of the function.
					funcret_oaiMsg = {
							'role':		'function',
							'name':		function_name,
							'content':	result
						}

					# Finish building the message list. So, the sequence here is:
					#
					#	system:		[NOTE: ... is doing function call ...]
					#	assistant:	(function call)
					#	assistant:	{remark emitted by bot, if any}
					#   function:	(function return)

					temp_chat_oaiMsgs += [funcall_oaiMsg]
					temp_chat_oaiMsgs += trailing_oaiMsgs
					temp_chat_oaiMsgs += [funcret_oaiMsg]
					temp_chat_oaiMsgs += [{
							'role':		'system',
							'content':	f"{BOT_NAME} now provides its response, if any, to the function's return value:",
						}]
					
					# Display the most recent 10 chat messages from temp list.
					_logger.debug(f"Last few chat messages are [\n{pformat(temp_chat_oaiMsgs[-10:])}\n].")

					# We'll just do a quick-and-dirty approach here to the context length management.
					while True:
						try:
							# Do a dummy 2nd API call with the result.
							second_chatCompl = gptCore.genChatCompletion(
								messages 		= temp_chat_oaiMsgs,
								functionList	= functions,
							)
							break
						except PromptTooLargeException:
							# Just trim off the oldest message after the first two (time & system instructions).
							_logger.info(f"NOTE: Expunging oldest chat message:\n" + pformat(temp_chat_oaiMsgs[2]))
							temp_chat_oaiMsgs = temp_chat_oaiMsgs[0:2].extend(oaiMessages[3:])
							continue

					# Just for diagnostic purposes.
					_logger.info(f"GPT response to function return: [{pformat(second_chatCompl.message)}]")

					# Go ahead and add the danged thing. It better not be another function call though,
					# or empty, or trigger a content filter, or be a '/pass' command, because we just
					# aren't handling any of that here. Really need to rethink whole code structure.

					second_response_text = second_chatCompl.text

					_logger.info(f"Text of function return response: [{second_response_text}].")

					# If second response text has prompt in it, fix it.
					if second_response_text is not None:
						second_response_text = _trim_prompt(second_response_text)

					second_response_botMsg = BotMessage(conversation.bot_name, second_response_text)

					_logger.info(f"Resulting message object is: [{str(second_response_botMsg)}].")

					conversation.add_message(second_response_botMsg)

					if second_response_botMsg.text != '':	# Don't bother sending empty responses.

						# Is this even necessary in the case of chat engines?
						conversation.finalize_message(second_response_botMsg)

						# Process the AI's response to the function call's return.
						await process_response(update, context, second_response_botMsg)

					# NOTE: If the second response is a function call or a command,
					# we don't handle those cases properly here!  Fix this sometime.

				#__/ End of stubbed-out code for letting AI see and respond to the function return value.

				# At this point, we finished processing the function call. Just return.
				return

			#__/ End special code to handle function calls requested by the AI.


			# Also check for finish_reason == 'content_filter' and log/send a warning.
			finish_reason = chatCompletion.finishReason
			if finish_reason == 'content_filter':
				
				_logger.warn(f"OpenAI content filter triggered by user {user_name} " \
							 "(ID {user_id}) in chat {chat_id}. Response was:\n" + \
							 pformat(chatCompletion.chatComplStruct))

				WARNING_MSG = "WARNING: User {user_name} triggered OpenAI's content " + \
							  "filter. Repeated violations could result in a ban."

				# This allows the AI to see this warning message too.
				conversation.add_message(BotMessage(SYS_NAME, WARNING_MSG))

				repRes = await _reply_user(tgMsg, conversation, "[SYSTEM {WARNING_MSG}]")
				if repRes != 'success': return
				
			break	# We got a response, so we can break out of the loop.

		except PromptTooLargeException:				# Imported from gpt3.api module.

				# The prompt (constructed internally at the remote API back-end) is too long.  
				# Thus, we need to expunge the oldest message from the conversation.

			conversation.expunge_oldest_message()
				# NOTE: If it succeeds, this modifies conversation.context_string.

			# We've successfully expunged the oldest message.
			continue	# Loop back and try again.

		except RateLimitError as e:
			# This also may indicate that the server is overloaded
			# or our monthly quota was exceeded.

			# We exceeded our OpenAI API quota, or we've exceeded the rate limit 
			# for this model. There isn't really anything we can do here except 
			# send a diagnostic message to the user.

			_logger.error(f"Got a {type(e).__name__} from OpenAI ({e}) for conversation {chat_id}; aborting.")

			# Send a diagnostic message to the AI and to the user.
			diagMsg = "AI model is overloaded; please try again later."
			await _send_diagnostic(tgMsg, conversation, diagMsg)

			return	# That's all she wrote.
		#__/

		# Stuff from Copilot that we didn't use:
		#
		# except PromptTooLongException as e:
		#	  # The prompt was too long, so we need to shorten it.
		#	  # First, we'll log this at the INFO level.
		#	  _logger.info(f"Prompt too long; shortening it.")
		#	  # Then, we'll shorten the prompt and try again.
		#	  conversation.shorten_prompt()
		#	  continue
		# except RateLimitException as e:
		#	  # We've hit the rate limit, so we need to wait a bit before trying again.
		#	  # First, we'll log this at the INFO level.
		#	  _logger.info(f"Rate limit exceeded; waiting {e.retry_after} seconds.")
		#	  # Then, we'll wait for the specified number of seconds and try again.
		#	  time.sleep(e.retry_after)
		#	  continue

		# This one was also suggested by Copilot; we'll go ahead and use it.
		except Exception as e:
			# We've hit some other exception, so we need to log it and send
			# a diagnostic message to the user.
			# (And also add it to the conversation so the AI can see it.)
			
			await _report_error(conversation, tgMsg, f"Exception while "
								f"getting response: {type(e).__name__} ({e})")
			return
		#__/
	#__/

	# If we get here, we've successfully gotten a response from the API.

	# If the response is empty, then return early. (Can't even send an empty message anyway.)
	if response_text == "":

		_logger.warn("AI's text response was null. Ignoring...")

		## No longer needed because we don't add an empty message.
		# Delete the last message from the conversation.
		#conversation.delete_last_message()

		## Commenting this out for production.
		# # Send the user a diagnostic message indicating that the response was empty.
		# # (Doing this temporarily during development.)

		#diagMsg = "Response was empty."
		#await _send_diagnostic(message, conversation, diagMsg, toAI=False, ignore=True)
		
		return		# This means the bot is simply not responding to this particular message.
	
	# Generate a debug-level log message to indicate that we're starting a new response.
	_logger.debug(f"Creating new response from {conversation.bot_name} with "
				  f"text: [{response_text}].")

	# Create a new Message object and add it to the conversation.
	response_botMsg = BotMessage(conversation.bot_name, response_text)
	conversation.add_message(response_botMsg)

	# Strip off any leading or trailing whitespace (Telegram won't display it anyway.).
	response_text = response_text.strip()

	# Update the message object, and the context.
	response_botMsg.text = response_text
	conversation.expand_context()	 

	# If this message is already in the conversation, then we'll suppress it, so as
	# not to exacerbate the AI's tendency to repeat itself.	 (So, as a user, if you 
	# see that the AI isn't responding to a message, this may mean that it has the 
	# urge to repeat something it said earlier, but is holding its tongue.)
	if response_text.lower() != '/pass' and conversation.is_repeated_message(response_botMsg):

		# Generate an info-level log message to indicate that we're suppressing the response.
		_logger.info(f"Suppressing response [{response_text}]; it's a repeat.")

		# Delete the last message from the conversation.
		conversation.delete_last_message()

		## Send the user a diagnostic message (doing this temporarily during development).
		#diagMsg = f"Suppressing response [{response_text}]; it's a repeat."
		#await _send_diagnostic(message, conversation, diagMsg, toAI=False, ignore=True)
		
		return		# This means the bot is simply not responding to the message

	# If we get here, then we have a non-empty message that's also not a repeat.
	# It's finally OK at this point to archive the message and send it to the user.

	# Make sure the response message has been finalized (this also archives it).
	conversation.finalize_message(response_botMsg)

	# If we get here, we have finally obtained a non-empty, non-repeat,
	# already-archived message that we can go ahead and send to the user.
	# We also check to see if the message is a command line.

	await process_response(update, context, response_botMsg)	   # Defined below.

#__/ End of process_chat_message() function definition.


# Below are functions to implement commands that may performed by the AI.
# They are: remember, forget, block, and image.
#	* /remember <text> - Adds <text> to persistent memory.
#	* /forget <text> - Removes <text> from persistent memory.
#	* /block - Blocks the current user.
#	* /image <desc> - Generates an image with a given text description and sends it to the user.

# Define a function to handle the /remember command, when issued by the AI.
async def ai_remember(updateMsg:TgMsg, conversation:BotConversation, textToAdd:str) -> None:
	"""The AI calls this function to add the given text to its persistent memory."""

	# Put the message from the Telegram update in a convenient variable.
	message = updateMsg

	# Retrieve the conversation's chat ID.
	chat_id = conversation.chatID	# Public property. Type: int.

	# All the following code used to appear directly inside handle_response(),
	# but I've moved it here to make it easier to call it from other places,
	# such as the new code to handle function-call responses from the AI.

	# Check for missing <textToAdd> argument.
	if textToAdd == None:
		_logger.error(f"The AI sent a /remember command with no "
						f"argument in conversation {chat_id}.")

		diagMsg = "/remember command needs an argument."
		sendRes = await _send_diagnostic(message, conversation, diagMsg)
		if sendRes != 'success': return sendRes
		
		return "error: missing required argument"
	#__/


	# Tell the conversation object to add the given message to the AI's persistent memory.
	if not conversation.add_memory(textToAdd):
		
		errmsg = _lastError

		# Generate an error-level report to include in the application log.
		_logger.error(f"The AI tried & failed to add memory: [{textToAdd}]")

		diagMsg = f"Could not add [{textToAdd}] to persistent memory. " \
					f'Error message was: "{errmsg}"'

		# Send the diagnostic message to the user.
		sendRes = await _send_diagnostic(message, conversation, diagMsg)
		if sendRes != 'success': return sendRes

		return "error: unable to add memory item"
	#__/


	_logger.normal(f"\tThe AI added [{textToAdd}] to persistent memory in conversation {chat_id}.")

	# Also notify the user that we're remembering the given statement.
	diagMsg = f"Added [{textToAdd}] to persistent memory."

	# Send the diagnostic message to the user.
	sendRes = await _send_diagnostic(message, conversation, diagMsg)
	return sendRes

#__/ End of ai_remember() function definition.
				

# Define a function to handle the /forget command, when issued by the AI.
async def ai_forget(updateMsg:TgMsg, conversation:BotConversation, textToDel:str) -> None:
	"""The AI calls this function to remove the given text from its persistent memory."""

	# Put the message from the Telegram update in a convenient variable.
	message = updateMsg

	# Retrieve the conversation's chat ID.
	chat_id = conversation.chatID	# Public property. Type: int.

	# The following code used to appear directly inside handle_response(), but I've
	# moved it here to make it easier to call it from other places, such as the
	# code to handle function-call responses from the AI.

	# Check for missing <textToDel> argument.
	if textToDel == None:
		_logger.error(f"The AI sent a /forget command with no " \
					  f"argument in conversation {chat_id}.")

		diagMsg = "/forget command needs an argument."
		sendRes = await _send_diagnostic(message, conversation, diagMsg)
		if sendRes != 'success': return sendRes

		return "error: missing required argument"
	#__/


	# Tell the conversation object to remove the given message
	# from the AI's persistent memory.  The return value from
	# this call is True if the message was found and removed,
	# and False if it wasn't.

	if conversation.remove_memory(textToDel):

		# Log this at normal level.
		_logger.normal(f"\tThe AI removed [{textToDel}] from persistent memory in conversation {chat_id}.")

		# Also notify the AI & user that we're forgetting the given statement.
		diagMsg = f"Removed [{textToDel}] from persistent memory."
		sendRes = await _send_diagnostic(message, conversation, diagMsg)
		if sendRes != 'success': return sendRes
		
		return "success"	# Processed AI's /forget command successfully.

	else:

		errmsg = _lastError
		
		# Log this at ERROR level.
		_logger.error("The AI tried & failed to remove "
						f"[{textToDel}] from persistent memory in "
						f"conversation {chat_id}; error: {errmsg}.")

		# Also notify the user that we couldn't forget the given statement.
		diagMsg = f"Could not remove [{textToDel}] from persistent " \
					f'memory. Error message was: "{_lastError}"'
		sendRes = await _send_diagnostic(message, conversation, diagMsg)
		if sendRes != 'success': return sendRes
		
		return f"error: {errmsg}"

#__/ End of ai_forget() function definition.


# Define a function to handle the /block command, when issued by the AI.
async def ai_block(updateMsg:TgMsg, conversation:BotConversation, userToBlock:str=None) -> str:
	"""The AI calls this function to block the given user. If no user is specified,
		it blocks the current user (the one who sent the current update)."""
	
	# Put the message from the Telegram update in a convenient variable.
	message = updateMsg

	# Retrieve the current user's name, in case we need it.
	user_name = _get_user_name(message.from_user)

	# Retrieve the conversation's chat ID.
	chat_id = conversation.chatID	# Public property. Type: int.

	# The following code used to appear directly inside handle_response(), but I've
	# moved it here to make it easier to call it from other places, such as the
	# code to handle function-call responses from the AI.

	# If no user was specified, then we'll block the current user.
	if userToBlock == None:
		userToBlock = user_name

	# Generate a warning-level log message to indicate that we're blocking the user.
	_logger.warn(f"***ALERT*** The AI is blocking user '{userToBlock}' in conversation {chat_id}.")

	if _isBlocked(userToBlock):
		_logger.error(f"User '{userToBlock}' is already blocked.")
		diagMsg = f'User {userToBlock} has already been blocked by {BOT_NAME}.'
		return_msg = "User {userToBlock} is already blocked!"
	else:
		success = _blockUser(userToBlock)
		if success:
			diagMsg = f'{BOT_NAME} has blocked user {userToBlock}.'
			return_msg = "Success: blocked user {userToBlock}."
		else:
			error = _lastError	# Fetch the error message.
			await _report_error(conversation, message, error)
			return 'blocking the app developer is not allowed'
	
	# Send diagnostic message to AI and to user.
	sendRes = await _send_diagnostic(message, conversation, diagMsg)
	if sendRes != 'success': return sendRes

	return return_msg

#__/ End of ai_block() function definition.
				

# Define a function to handle the /unblock command, when issued by the AI.
async def ai_unblock(updateMsg:TgMsg, conversation:BotConversation, userToUnblock:str=None) -> str:
	"""The AI calls this function to unblock the given user. If no user is specified,
		it unblocks the current user (the one who sent the current update).
		(Note this case will normally never occur.)
	"""
	
	# Put the message from the Telegram update in a convenient variable.
	message = updateMsg

	# Retrieve the current user's name, in case we need it.
	user_name = _get_user_name(message.from_user)

	# Retrieve the conversation's chat ID.
	chat_id = conversation.chatID	# Public property. Type: int.

	# The following code used to appear directly inside handle_response(), but I've
	# moved it here to make it easier to call it from other places, such as the
	# code to handle function-call responses from the AI.

	# If no user was specified, then we'll unblock the current user.
	if userToUnblock == None:
		userToUnblock = user_name

	# Generate a warning-level log message to indicate that we're blocking the user.
	_logger.warn(f"***ALERT*** The AI is unblocking user '{userToUnblock}' in conversation {chat_id}.")

	if not _isBlocked(userToUnblock):
		_logger.error(f"User '{userToUnblock}' is not blocked.")
		diagMsg = f'User {userToUnblock} is not currently blocked by {BOT_NAME}.'
		return_msg = "User {userToUnblock} is not blocked!"
	else:
		# This always succeeds.
		_unblockUser(userToUnblock)
		diagMsg = f'{BOT_NAME} has unblocked user {userToUnblock}.'
		return_msg = "Success: unblocked user {userToUnblock}."
	
	# Send diagnostic message to AI and to user.
	sendRes = await _send_diagnostic(message, conversation, diagMsg)
	if sendRes != 'success': return sendRes

	return return_msg

#__/ End of ai_unblock() function definition.
				

# Define a function to handle the /image command, when issued by the AI.
async def ai_image(update:Update, context:Context, imageDesc:str, caption:str=None	#, remaining_text:str=None
	) -> None:

	# Get the message, or edited message from the update.
	(message, edited) = _get_update_msg(update)

	# Get the chat_id, user_name, and conversation object.
	chat_id = message.chat.id
	user_name = _get_user_name(message.from_user)
	conversation = context.chat_data['conversation']

	# Error-checking for null argument.
	if imageDesc == None or imageDesc=="":
		_logger.error(f"The AI sent an /image command with no argument in conversation {chat_id}.")

		diagMsg = "/image command needs an argument."
		sendRes = await _send_diagnostic(message, conversation, diagMsg)
		if sendRes != 'success': return sendRes

		return "error: null image description"

	# Generate and send an image described by the /image command argument string.
	_logger.normal("\nGenerating an image with description "
					f"[{imageDesc}] for user '{user_name}' in "
					f"conversation {chat_id}.")
	if caption:
		_logger.normal(f"\tAn image caption [{caption}] was also specified.")

	await send_image(update, context, imageDesc, caption=caption)

	# Make a note in conversation archive to indicate that the image was sent.
	conversation.add_message(BotMessage(SYS_NAME, f'[Generated and sent image "{imageDesc}"]'))

	## NOTE: This is now done in process_command() more generically.
	# Send the remaining text after the command line, if any, as a normal message.
	#if remaining_text != None and remaining_text != '':
	#	await send_response(update, context, remaining_text)

	return "success: image has been generated and sent to user"

#__/ End of ai_image() function definition.


async def ai_call_function(update:Update, context:Context, funcName:str, funcArgs:dict) -> str:

	"""Call the named AI-available function with the given argument dict. Returns a result string."""

	# Get the user message, or edited message from the update.
	(message, edited) = _get_update_msg(update)
	
	# Get the chat_id, user_name, and conversation object.
	chat_id = message.chat.id
	user_name = _get_user_name(message.from_user)
	conversation = context.chat_data['conversation']
	
	# Dispatch on the function name. See FUNCTIONS_LIST.
	if funcName == 'remember_item':
		textToAdd = funcArgs.get('item_text', None)

		if textToAdd:
			return await ai_remember(message, conversation, textToAdd)
		else:
			await _report_error(conversation, message,
					f"remember_item() missing required argument item_text.")
			return "error: required argument item_text is missing"

	elif funcName == 'forget_item':
		textToDel = funcArgs.get('item_text', None)

		if textToDel:
			return await ai_forget(message, conversation, textToDel)
		else:
			await _report_error(conversation, message,
					f"forget_item() missing required argument item_text.")
			return "error: required argument item_text is missing"

	elif funcName == 'create_image':
		imageDesc = funcArgs.get('image_desc', None)
		caption = funcArgs.get('caption', None)

		if imageDesc:
			return await ai_image(update, context, imageDesc, caption)
		else:
			await _report_error(conversation, message,
					f"create_image() missing required argument image_desc.")
			return "error: required argument image_desc is missing"

	elif funcName == 'block_user':
		userToBlock = funcArgs.get('user_name', user_name)		# Default to current user.

		return await ai_block(message, conversation, userToBlock)

	elif funcName == 'unblock_user':
		userToUnblock = funcArgs.get('user_name', user_name)		# Default to current user.

		return await ai_unblock(message, conversation, userToUnblock)

	elif funcName == 'pass_turn':
		_logger.normal(f"\nNOTE: The AI is passing its turn in conversation {chat_id}.")
		return None		# Just do nothing; no return.

	else:
		await _report_error(conversation, message,
							f"AI tried to call an undefined function '{funcName}()'.")
		return f"error: {funcName} is not an available function"

#__/ End definition of private function ai_call_function().


# Process a command (message starting with '/') from the AI.
async def process_ai_command(update:Update, context:Context, response_text:str) -> None:
	"""Given the text of a message returned by the AI, where that text starts
		with a '/' character, this function interprets it as an attempt by the
		AI to issue a command, and handles this appropriately."""

	# Get the user message, or edited message from the update.
	(message, edited) = _get_update_msg(update)
	
	# Get the chat_id, user_name, and conversation object.
	chat_id = message.chat.id
	user_name = _get_user_name(message.from_user)
	conversation = context.chat_data['conversation']

	# Extract the command name from the message.  We'll do this
	# with a regex that captures the command name, and then the
	# rest of the command line.  But first, we'll capture just the
	# first line of the message, followed by the rest.

	# Split the text into lines
	lines = response_text.splitlines()

	# Get the first line
	first_line = lines[0]

	# Output the command line to the console.
	_logger.normal("The AI sent a command line: " + first_line)

	# Get the remaining text by joining the rest of the lines
	remaining_text = '\n'.join(lines[1:])

	# Now, we'll use a regex to parse the command line to
	# extract the command name and optional arguments.
	match = re.match(r"^/(\S+)(?:\s+(.*))?$", first_line)

	# Extract the command name and arguments from the match.
	command_name = command_args = None
	if match is not None:
		groups = match.groups()
		command_name = groups[0]
		if len(groups) > 1:
			command_args = groups[1]

	## Now, we'll process the command.

	# First, let's go ahead and show the command line to the user... (Ignoring send errors.)
	await _reply_user(message, conversation, first_line, ignore=True)

	# NOTE: We can't just call the normal command handlers
	# directly, because they are designed for commands issued by
	# the user, not by the AI. So, we'll have to process the
	# commands ourselves to handle them correctly.

	# Check to see if the AI typed the '/remember' command.
	if command_name == 'remember':
		# This is a command to remember something.

		_logger.normal(f"\nAI {BOT_NAME} entered a /remember command in chat {chat_id}.")

		# Should we issue a warning here if there is remaining text
		# after the command line that we're ignoring?  Or should we 
		# include the remaining text as part of the memory to be 
		# remembered?

		# This does all the work of handling the '/remember' command
		# when issued by the AI.
		await ai_remember(message, conversation, command_args)

	# Check to see if the AI typed the '/forget' command.
	elif command_name == 'forget':
		# This is a command to forget something.

		_logger.normal(f"\nAI {BOT_NAME} entered a /forget command in chat {chat_id}.")

		# Should we issue a warning here if there is remaining text
		# after the command line that we're ignoring?  Or should we 
		# include the remaining text as part of the memory to be 
		# forgotten?

		# This does all the work of handling the '/forget' command
		# when issued by the AI.
		await ai_forget(message, conversation, command_args)

	elif command_name == 'block':
		# Adds the current user (or a specified user) to the block list.

		_logger.normal(f"\nAI {BOT_NAME} entered a /block command in chat {chat_id}.")

		# Should we issue a warning here if there is remaining text
		# after the command line that we're ignoring?  Or should we
		# send the remaining text as a normal message?

		# This does all the work of handling the '/block' command
		# when issued by the AI.
		await ai_block(message, conversation, command_args)

	elif command_name == 'unblock':
		# Removes the current user (or a specified user) from the block list.

		_logger.normal(f"\nAI {BOT_NAME} entered an /unblock command in chat {chat_id}.")

		# Should we issue a warning here if there is remaining text
		# after the command line that we're ignoring?  Or should we
		# send the remaining text as a normal message?

		# This does all the work of handling the '/unblock' command
		# when issued by the AI.
		await ai_unblock(message, conversation, command_args)

	elif command_name == 'image':
		# This is a command to generate an image and send it to the user.
		
		_logger.normal(f"\nAI {BOT_NAME} entered an /image command in chat {chat_id}.")
			
		# This does all the work of handling the '/image' command
		# when issued by the AI.
		await ai_image(update, context, command_args)
			# NOTE: We're passing the entire update object here, as well
			# as the context, because we need to be able to send a message
			# to the user, and we can't do that with just the message object.

		# NOTE: We could have passed remaining_text as the 4th argument (caption),
		# but it's perhaps better to just send it as an ordinary text message.

	else:
		# This is a command type that we don't recognize.
		_logger.warn(f"\nAI {BOT_NAME} entered an unknown command [/{command_name}] in chat {chat_id}.")

		# Send the user a diagnostic message.
		diagMsg = f"Unknown command [/{command_name}]."
		await _send_diagnostic(message, conversation, diagMsg, ignore=True)

	# If there was any additional text remaining after the command line,
	# just send it as a normal message.
	if remaining_text != "":
		await send_response(update, context, remaining_text)

	# At this point we are done processing the command.
#__/


async def process_response(update:Update, context:Context, response_botMsg:BotMessage) -> None:
	"""Given a message object (of our Message class) representing a response
		issued by the AI to some user message, this function processes it
		appropriately; it may be interpreted as a text command issued by the
		AI, or as a normal message to be sent to the user."""

	# Get the user message, or edited message from the update.
	(tgMsg, edited) = _get_update_msg(update)
	
	# Get the chat_id, user_name, and conversation object.
	chat_id = tgMsg.chat.id
	#user_name = _get_user_name(tgMsg.from_user)
	#conversation = context.chat_data['conversation']
	response_text = response_botMsg.text

	# First, check to see if the AI typed the '/pass' command, in which case we do nothing.
	if response_text.lower() == '/pass':
		_logger.normal(f"\nNOTE: The AI is passing its turn in conversation {chat_id}.")
		return

	if response_text == "":		# Response is empty string??
		_logger.error(f"In process_response(), got an empty response message: [{str(response_botMsg)}].")
		return

	# Finally, we check to see if the AI's message is a command line;
	# that is, if it starts with '/' followed by an identifier (e.g.,
	# '/remember').  If so, we'll process it as a command.
	if response_text[0] == '/':

		# The AI attempted to send a command line, so process it as such.
		await process_ai_command(update, context, response_text)

		return

	else: # Response was not a command. Treat it normally.

		# Just send our response to the user as a normal message.
		await send_response(update, context, response_text)

	# One more thing to do here: If the AI's response ends with the string "(cont)" or "(cont.)"
	# or "(more)" or "...", then we'll send a message to the user asking them to continue the 
	# conversation.
	if response_text.endswith("(cont)") or response_botMsg.text.endswith("(cont.)") or \
	   response_text.endswith("(more)") or response_botMsg.text.endswith("..."):

		contTxt = "[If you want me to continue my response, type '/continue'.]"
		await _reply_user(tgMsg, None, contTxt, toAI=False, ignore=True)
	#__/

	# Processed AI's response successfully.

#__/ End of process_response() function definition.


async def send_image(update:Update, context:Context, desc:str, caption=None, save_copy=True) -> None:
	"""Generates an image from the given description and sends it to the user.
		Also archives a copy on the server unless save_copy=False is specified."""

	# Get the message, or edited message from the update.
	(tgMsg, edited) = _get_update_msg(update)
		
	if tgMsg is None:
		_logger.warning("In send_image() with no message? Aborting.")
		return

	# Get the message's chat ID.
	chat_id = tgMsg.chat.id

	# Get our preferred name for the user.
	username = _get_user_name(tgMsg.from_user)

	# Get our Conversation object.
	conversation = context.chat_data['conversation']

	_logger.normal(f"\tGenerating image for user {username} from " \
				   f"description [{desc}]. Caption is [{str(caption)}]...")

	# Use the OpenAI API to generate the image.
	try:
		image_url = genImage(desc)
	except Exception as e:
		await _report_error(conversation, tgMsg,
					  f"In send_image(), genImage() threw an exception: {type(e).__name__} ({e})")

		# We could also do a traceback here. Should we bother?
		raise

	_logger.normal(f"\tDownloading generated image from url [{image_url[0:50]}...]")

	# Download the image from the URL
	response = requests.get(image_url)
	response.raise_for_status()
	
	# Save the image to the filesystem if the flag is set to True
	if save_copy:
		_logger.normal(f"\tSaving a copy of the generated image to the filesystem...")
		image_dir = os.path.join(AI_DATADIR, 'images')
		if not os.path.exists(image_dir):
			os.makedirs(image_dir)
		# Pick a short ID for the file (collisions will be fairly rare).
		short_file_id = f"{random.randint(1,1000000)-1:06d}"
		image_save_path = os.path.join(image_dir, f'{username}--{short_file_id}.png')
		with open(image_save_path, 'wb') as image_file:
			image_file.write(response.content)
		_logger.normal(f"\t\tImage saved to {image_save_path}.")

	_logger.normal(f"\tSending generated image to user {username}...")

	# Prepare the image to be sent via Telegram
	image_data = InputFile(response.content)
	
	# Send the image as a reply in Telegram
	try:
		await tgMsg.reply_photo(photo=image_data, caption=caption)
	except BadRequest or Forbidden or ChatMigrated as e:
		_logger.error(f"Got a {type(e).__name__} exception from Telegram "
					  "({e}) for conversation {chat_id}; aborting.")
		conversation.add_message(BotMessage(SYS_NAME, "[ERROR: Telegram " \
			"exception {exType} ({e}) while sending to user {user_name}.]"))
	#__/
#__/


async def send_response(update:Update, context:Context, response_text:str) -> None:
	
	# Get the message, or edited message from the update.
	(message, edited) = _get_update_msg(update)
		
	if message is None:
		_logger.warning("In send_response() with no message? Aborting.")
		return

	chat_id = message.chat.id

	# Now, we need to send the response to the user. However, if the response is
	# longer than the maximum allowed length, then we need to send it in chunks.
	# (This is because Telegram's API limits the length of messages to 4096 characters.)

	MAX_MESSAGE_LENGTH = 4096	# Maximum length of a message. (Telegram's API limit.)
		# NOTE: Somwhere I saw that 9500 was the maximum length of a message, but I don't know
		#	which is the correct maximum.

	# Send the message in chunks.
	while len(response_text) > MAX_MESSAGE_LENGTH:
		await _reply_user(message, None, response_text[:MAX_MESSAGE_LENGTH])
		response_text = response_text[MAX_MESSAGE_LENGTH:]

	# Send the last chunk.
	await _reply_user(message, None, response_text)
#__/


		#/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		#|	The following function will be used to display the current
		#|  date/time to the AI, including the time zone.
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# The following function will get the current date/time as a string, including the timezone.
def timeString() -> str:

	"""Returns a nicely-formatted string representing the current date, time,
		and timezone."""

	dateTime = tznow()	# Function to get the current date and time in the local timezone.
	fmtStr = _TIME_FORMAT  # The base format string to use.

	# Is the 'TZ' environment variable set?
	#	If so, then we can add '(%Z)' (time zone abbreviation) to the format str.
	if envTZ is not None:
		fmtStr = fmtStr + " (%Z)"
	
	timeStr = dateTime.strftime(fmtStr)	 # Format the date/time string.

	# If 'TZ' was not set, then we have to try to guess the time zone name from the offset.
	if envTZ is None:
		tzAbb = tzAbbr()	# Function to get the time zone abbreviation from the offset.
		timeStr = timeStr + f" ({tzAbb})"

	return timeStr
#__/


	#|==========================================================================
	#|	4.2. Misc. minor/private functions.			[python module code section]
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

def _blockUser(user:str) -> bool:
	"""Blocks the given user from accessing the bot.
		Returns True if successful; False if failure."""

	global _lastError
	
	ai_datadir = AI_DATADIR

	# If the AI is trying to block the Creator, don't let him.
	if user == 'Michael':
		_logger.error("The AI tried to block the app developer! Disallowed.")
		_lastError = "Blocking the bot's creator, Michael, is not allowed."
		return False

	block_list = []

	bcl_file = os.path.join(ai_datadir, 'bcl.json')
	if os.path.exists(bcl_file):
		with open(bcl_file, 'r') as f:
			block_list = json.load(f)
	
	if user in block_list:
		_logger.warn(f"_blockUser(): User {user} is already blocked. Ignoring.")

	block_list.append(user)
	with open(bcl_file, 'w') as f:
		json.dump(block_list, f)

	return True
#__/
	

def _call_desc(func_name:str, func_args:dict):
	"""Convert a function name and argument dictionary to a string
		representing the function call."""

	# Generate a description of the function call, for diagnostic purposes.
	kwargstr = ', '.join([f'{key}="{value}"' for key, value in func_args.items()])
	call_desc = f"{func_name}({kwargstr})"
	return call_desc
#__/


# This function checks whether the given user name is in our access list.
# If it is, it returns True; otherwise, it returns False.

def _check_access(user_name, prioritize_bcl=True) -> bool:

	"""Returns True if the given user may access the block.
		Blacklist (bcl.[h]json) overrides whitelist (acl.hjson)
		unless prioritize_bcl=False is specified."""

	if prioritize_bcl:
		# Temporary override to have blacklist override whitelist.
		return not _isBlocked(user_name)

	# Get the value of environment variable AI_DATADIR.
	# This is where we'll look for the access list file.
	ai_datadir = AI_DATADIR

	# Now look for the file "acl.hjson" in the AI_DATADIR directory.
	# If it exists, then we'll use it as our access list.
	# If it doesn't exist, then we'll allow access to everyone except
	# who's in the blocklist.

	acl_file = os.path.join(ai_datadir, 'acl.hjson')
	if os.path.exists(acl_file):

		# Use the hjson module to load the access list file.
		# (This is a JSON-like file format that allows comments.)
		#import hjson	# NOTE: We already imported this at the top of the file.
		with open(acl_file, 'r') as f:
			access_list = hjson.load(f)			# Load the file into a Python list.

		# If the access list is empty, then we allow access to everyone.
		if len(access_list) > 0:

			# Otherwise, check whether the user name is in the access list.
			if user_name in access_list:
				return True
			else:
				return False
		#__/
	#__/

	# If we get here, either there's no acl or the acl is empty.
	# Allow the user unless they're in the blocklist.

	bcl_file = os.path.join(ai_datadir, 'bcl.hjson')
	if not os.path.exists(bcl_file):
		return True

	with open(bcl_file, 'r') as f:
		block_list = hjson.load(f)
		if user_name in block_list:
			return False

	# If we get here, there's no ACL and the user isn't blocked, so allow them.
	return True	

#__/ End definition of private function _check_access().


async def _ensure_convo_loaded(update:Update, context:Context) -> bool:

	"""Helper function to ensure the conversation data is loaded,
		and auto-restart the conversation if isn't."""

	# Get the message, or edited message from the update.
	(message, edited) = _get_update_msg(update)
		
	if message is None:
		_logger.warning("In _ensure_convo_loaded() with no message? Aborting.")
		return False

	# Get the chat ID.
	chat_id = message.chat.id

	# Get the user's name.
	user_name = _get_user_name(message.from_user)

	if not 'conversation' in context.chat_data:

		_logger.normal(f"\nUser {user_name} sent a message in an uninitialized conversation {chat_id}.")
		_logger.normal(f"\tAutomatically starting (or restarting) conversation {chat_id}.")

		diagMsg = "Either this is a new chat, or the bot server was rebooted. Auto-starting conversation."
			# NOTE: The AI won't see this diagnostic because the convo hasn't even been reloaded yet!

		sendRes = await _send_diagnostic(message, None, diagMsg, toAI=False)
		if sendRes != 'success': return False	# Callers expect Boolean

		await handle_start(update, context, autoStart=True)
		# We assume it succeeds.

	#__/
	
	return True
#__/


# This function extracts the edited_message or message field from an update,
# as appropriate, and returns the pair (message, edited).
def _get_update_msg(update:Update):
	edited = False
	message = None
	if update.edited_message is not None:
		message = update.edited_message
		edited = True
	elif update.message is not None:
		message = update.message
	return (message, edited)


# This function, given a Telegram user object, returns a string that identifies the user.
def _get_user_name(user) -> str:

	"""Return a string that identifies the given Telegram user."""

	global _which_name

	# Decide what we'll call this user. We'll use their first_name attribute, unless it
	# is None, or an empty string, or contains non-identifier characters, in which case 
	# we'll use their username attribute. If that's also None, we'll use their ID. 

	user_name = user.first_name
		# By default, we'll use the user's first name.
	_which_name = 'first name'

	# If the user's first name is an empty string, we'll invalidate it by setting it to None.
	if user_name == '':
		user_name = None

	# If we're using the GPT-3 Chat API, we'll also need to make sure that the user's name
	# is a valid identifier that's accepted by that API as a user name.
	if user_name is not None and gptCore.isChat:
		# If the user's first name contains characters the GPT-3 Chat API won't accept 
		# (or is too long or an empty string), we'll invalidate it by setting it to None.
		if not re.match(r"^[a-zA-Z0-9_-]{1,64}$", user_name):
			user_name = None

	# If the user's first name wasn't valid, we'll try to use their username.
	if user_name is None:
		user_name = user.username
		_which_name = 'username'

		# But, if that name isn't valid either, we'll use their ID.
		if user_name is None or user_name == '':
			user_name = str(user.id)
			_which_name = 'user ID'

	return user_name
#__/


	#----------------------------------------------------------------------
	# This function initializes the AI's persistent context information
	# based on the PERSISTENT_DATA string. We'll call it whenever the
	# PERSISTENT_DATA string changes, which will happen when we read the
	# AI's persistent memory file, or when a '/remember' command is issued.
	
def _initPersistentContext() -> None:

	global globalPersistentData, globalPersistentContext	# So we can modify these.

	# Initialize the AI's persistent context information.
	
	## No longer needed because we now give a command menu even 
	## when the AI has functions as well.
	#
	#if hasFunctions(ENGINE_NAME):
	#	globalPersistentContext = \
	#		MESSAGE_DELIMITER + PERMANENT_CONTEXT_HEADER + \
	#		globalPersistentData + \
	#		MESSAGE_DELIMITER + RECENT_MESSAGES_HEADER
	#	#__/
	#else:

	globalPersistentContext = \
		MESSAGE_DELIMITER + PERMANENT_CONTEXT_HEADER + \
			globalPersistentData + \
		MESSAGE_DELIMITER + COMMAND_LIST_HEADER + \
			"  /pass - Refrain from responding to the last user message.\n" + \
			"  /image <desc> - Generate an image with description <desc> and send it to the user.\n" + \
			"  /remember <text> - Adds <text> to my persistent context data.\n" + \
			"  /forget <text> - Removes <text> from my persistent context data.\n" + \
			"  /block [<user>] - Adds the user to my block list. Defaults to current user.\n" + \
			"  /unblock [<user>] - Removes the user from my block list. Defaults to current user.\n" + \
		MESSAGE_DELIMITER + RECENT_MESSAGES_HEADER
	#__/
#__/


		#-----------------------------------------------------------------------
		# Initialize the bot's persistent data string, including any dynamical-
		# ly-added persistent memories.
		#
		# NOTE: The memory/context stuff in globals really needs to be augmented
		# w. additional memories that are specific to an individual conversation
		# and/or user.

def _initPersistentData() -> None:
	"""Initialize the persistent data string (including memories, if any)."""

	global globalPersistentData

	# This function initializes the AI's persistent context data.

	# Initialize the main data for the AI's persistent context.
	globalPersistentData = aiConf.context 
		# NOTE: This should end with a newline. But if it doesn't, we'll add one.

	# Ensure that PERSISTENT_DATA ends with a newline.
	if globalPersistentData[-1] != '\n':
		globalPersistentData += '\n'

	# Append current memories, if any.
	if MEMORIES != "":
		globalPersistentData += MESSAGE_DELIMITER + PERSISTENT_MEMORY_HEADER
		globalPersistentData += MEMORIES

	# NOTE: The MEMORIES variable is intended for global (but dynamic) memories
	# for the current bot. Eventually we need to also add a section for user-
	# specific and/or chat-specific memories.
#__/ End definition of _initPersistentData() function.

	
def _isBlocked(user:str) -> bool:
	"""Return True if user is on blacklist, or if there
		is a whitelist and the user is not on it."""
	
	# Get the value of environment variable AI_DATADIR.
	# This is where we'll look for the block list files.
	ai_datadir = AI_DATADIR
	
	# User is blocked if they're in the bcl.hjson file.

	bcl_file = os.path.join(ai_datadir, 'bcl.hjson')
	if os.path.exists(bcl_file):
		with open(bcl_file, 'r') as f:
			block_list = hjson.load(f)
			if user in block_list:
				return True
		
	# User is blocked if they're in the bcl.json file.
	
	bcl_file = os.path.join(ai_datadir, 'bcl.json')
	if os.path.exists(bcl_file):
		with open(bcl_file, 'r') as f:
			block_list = json.load(f)
			if user in block_list:
				return True

	# Not in either file? Check to see if there's an ACL (whitelist) and user is not in it.

	acl_file = os.path.join(ai_datadir, 'acl.hjson')
	if os.path.exists(acl_file):	# If whitelist doesn't exist, user isn't blocked.
		
		with open(acl_file, 'r') as f:
			access_list = hjson.load(f)			# Load the file into a Python list.

		# If the whitelist is empty, then we allow access to everyone.
		if len(access_list) > 0:

			# Otherwise, check whether the user name is in the access list.
			if user in access_list:
				return False	# User is not blocked.
			else:
				return True		# There's a whitelist but user isn't in it. Effectively blocked.

	return False
#__/


# Sends a message to the user, with some appropriate exception handling.
# Returns 'success' if the send succeeded, or an error string if it failed.
# If ignore=True, then the error string indicates that the error is being
# ignored by the program.
async def _reply_user(userTgMessage:TgMsg, convo:BotConversation,
					  msgToSend:str, ignore:bool=False) -> str:

	"""Sends text message <msgToSend> in reply to the user's
		Telegram message <userTgMessage> in conversation <convo>."""

	message = userTgMessage		# Shorter name.

	# Get the user name.
	user_name = _get_user_name(message.from_user)

	# Get the chat ID from the conversation (if supplied) or the message.
	if convo is None:
		chat_id = message.chat.id
	else:
		chat_id = convo.chat_id

	# Try sending the message to the user.
	try:
		await message.reply_text(msgToSend)

	except BadRequest or Forbidden or ChatMigrated as e:

		exType = type(e).__name__

		whatDoing = "ignoring" if ignore else "aborting"

		_logger.error(f"Got a {exType} exception from Telegram ({e}) "
					  f"for conversation {chat_id}; {whatDoing}.")

		if convo is not None:
			convo.add_message(BotMessage(SYS_NAME, "[ERROR: Telegram exception " \
				"{exType} ({e}) while sending to user {user_name}.]"))

		# Note: Eventually we need to do something smarter here -- like, if we've
		# been banned from replying in a group chat or something, then leave it.

		return "error: Telegram threw a {exType} exception while sending " \
			"diagnostic output to the user"
	#__/

	return 'success'
#__/


async def _report_error(convo:BotConversation, telegramMessage,
				 errMsg:str, logIt:bool=True,
				 showAI:bool=True, showUser:bool=True) -> None:

	"""Report a given error response to a Telegram message. Flags
		<logIt>, <showAI>, <showUser> control where the error is
		reported."""

	chat_id = convo.chat_id

	if logIt:
		# Record the error in the log file.
		#_logger.error(errMsg, exc_info=logmaster.doDebug)
			# The exc_info option includes a stack trace if we're in debug mode.
		_logger.error(errMsg, exc_info=True)

	# Compose formatted error message.
	msg = f"ERROR: {errMsg}"

	if showAI:
		# Add the error message to the conversation.
		convo.add_message(BotMessage(SYS_NAME, msg))

	if showUser:
		await _reply_user(telegramMessage, convo, f"[SYSTEM {msg}]")

#__/ End private function _report_error().


# Sends a diagnostic message to the AI as well as to the user,
# with some appropriate exception handling. Returns 'success'
# if the send succeeded, or an error string if it failed.
# If toAI=False, we skip sending the message to the AI.
async def _send_diagnostic(userTgMessage:TgMsg, convo:BotConversation,
						   diagMsg:str, toAI=True, ignore:bool=False) -> str:
	"""Sends diagnostic message <diagMsg> in reply to the user's
		Telegram message <userTgMessage> in conversation <convo>.
		This function first adds the message to the convo. If 
		ignore=True then send failures are reported as ignored."""

	# Compose the full formatted diagnostic message.
	fullMsg = f"[DIAGNOSTIC: {diagMsg}]"

	# First, record the diagnostic for the AI's benefit.
	if toAI:
		convo.add_message(BotMessage(SYS_NAME, fullMsg))

	# Now also send it to the user.
	return await _reply_user(userTgMessage, convo, fullMsg, ignore)
#__/


def _trim_prompt(response_text:str) -> str:
	"""Trims the prompt portion off the front of the given
		response text, if present."""

	# Check to see if text response starts with a line that's
	# formatted like
	#
	#		MESSAGE_DELIMITER + " (sender)> (text)"
	#
	# or (if delimiter is null) "(sender)> (text)".  If so,
	# and if the sender is the bot itself (as expected),
	# trim the prompt part off the front.

	# Regex to match the prompt portion at the start of a message string.
	if MESSAGE_DELIMITER != "":
		regex = f"({MESSAGE_DELIMITER} ?)" + r"([a-zA-Z0-9_-]{1,64})> "
	else:
		regex = r"([a-zA-Z0-9_-]{1,64})> "
	# Note we don't need to start the regex with '^' because re.match()
	# only matches at the start of a string anyway.

	#_logger.normal("Using regex: [" + regex + "]")

	firstline = response_text.split('\n')[0]

	_logger.debug(f"Matching regex against: [{firstline}]")
	
	match = re.match(regex, firstline)

	if match:

		if MESSAGE_DELIMITER != "":
			prefix = match.group(1)
			sender = match.group(2)
		else:
			prefix = ""
			sender = match.group(1)

		_logger.debug(f"AI output a message from [{sender}]...")
		if sender == BOT_NAME:

			# Trim the sender and prompt part off of the front of the message text.
			toTrim = prefix + sender + '> '
			_logger.debug(f"Trimming this part off the front: [{toTrim}]")
			rest = response_text[len(toTrim):]
			response_text = rest
			
			_logger.debug(f"Now we are left with [{response_text}]...")
			
		#__/
	#__/

	return response_text

def _unblockUser(user:str) -> bool:
	"""Removes the given user from the bot's block list.
		Returns True if successful; False if failure."""

	ai_datadir = AI_DATADIR

	block_list = []

	bcl_file = os.path.join(ai_datadir, 'bcl.json')
	if os.path.exists(bcl_file):
		with open(bcl_file, 'r') as f:
			block_list = json.load(f)
	
	if user not in block_list:
		_logger.warn(f"_blockUser(): User {user} is not blocked. Ignoring.")

	block_list.remove(user)
	with open(bcl_file, 'w') as f:
		json.dump(block_list, f)

	return True
#__/
	

#/=============================================================================|
#|	5. Define globals.														   |
#|																			   |
#|		In this section, we define important global variable and con-		   |
#|		stant values and objects.									   		   |
#|																		   	   |
#|		NOTE: We really should rethink the structure of the program so		   |
#|		that fewer things have to go in globals in general.					   |
#|																		   	   |
#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv|

	#/======================================================================
	#|	5.1. Define global constants.		[python module code subsection]
	#|
	#|		By convention, we define global constants in all-caps.
	#|		(However, some of them may still end up being variable.)
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

		#/============================================================
		#| Hard-coded constants.
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	# We'll use this to delimit the start of each new message event in the AI's receptive field.

#MESSAGE_DELIMITER = '🤍'	# A Unicode character. Gladys selected the white heart emoji.
	# We're temporarily trying a different delimiter that's less likely to appear in message text:
#MESSAGE_DELIMITER = chr(ascii.RS)	# (Gladys agreed to try this.)
	# A control character.	(ASCII RS = 0x1E, record separator.)
#MESSAGE_DELIMITER = chr(ascii.ETX)	# End-of-text control character.
#MESSAGE_DELIMITER = chr(ascii.ETB)	# End-of-transmission-block control character.
MESSAGE_DELIMITER = ""				# No delimiter at all!
	# ^ Trying this in desperation to hopefully get rid of API errors.
	# NOTE: I think this was unnecessary.

	# This is the size, in messages, of the window at the end of the conversation 
	# within which we'll exclude messages in that region from being repeated by the AI.
	# (This is basically a hack to try to suppress the AI's tendency to repeat itself.)

NOREPEAT_WINDOW_SIZE = 10

	# This is the name we'll attach to messages generated by the system,
	# so the AI knows where they came from.

SYS_NAME = 'BotServer'	  # This refers to the present system, i.e., the Telegram bot server program.

# Time format string to use (note minutes are included, but not seconds).
_TIME_FORMAT = "%A, %B %d, %Y, %I:%M %p"
	# Format like "Saturday, June 10, 2023, 5:03 pm".

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#  Sets the stop sequence (terminates response when encountered).

# Configure the stop sequence appropriate for this application.
#stop_seq = MESSAGE_DELIMITER	# This is appropriate given the RS delimiter.
#stop_seq = ['\n' + MESSAGE_DELIMITER]	# Needed if delimiter might be in text.
	# NOTE: The stop parameter is used to tell the API to stop generating 
	# tokens when it encounters the specified string(s). We set it to stop 
	# when it encounters the message delimiter string at the start of a new 
	# line, which is used (in the pre-chat GPT-3 API) to separate messages 
	# in the conversation. This will prevent the AI from generating a 
	# response that includes the message delimiter string, which could then 
	# be followed by a hallucinated response from another user to the AI's 
	# own response, which would be very confusing. 
	# 
	# However, a more sophisticated way to handle this would be to use no 
	# stop sequence, and instead specifically check the output for the 
	# prompt sequence
	# 
	#		'\n' <MESSAGE_DELIMITER> <USER_OR_AI_NAME> '>',
	#  
	# and if the AI predicts another message from itself, then we could go 
	# ahead and process it as a separate Telegram message, so that the AI 
	# is then generating a string of several output messages in a row 
	# without needing additional prompting.	 Whereas, if/when the AI tries 
	# to predict a message from another user, then we could terminate its 
	# output at that point and just ignore the rest.
	#
	# Another important remark is that in the new GPT-3 chat API, all this 
	# is somewhat mooted, because prior messages in the conversation are 
	# automatically included in the context for the AI's response with 
	# suitable delimiters and new messages are cut off automatically, so 
	# there is no need really to include our special delimiters in the stop 
	# sequence, except to prevent the AI from hallucinating header block 
	# subsections, which also use the same delimiter string.

# Commented out the below because this logic is not really needed, due to 
# the last couple of lines above.  But I'm leaving it here for now in case
# we decide to prevent header-block hallucinations differently in the future. 

# # Temporary hack here that breaks encapsulation a bit to allow us to set the
# # stop sequence for the core connection object appropriately depending on the
# # engine type. Need to think about what would be a more elegant way to do this.
# import gpt3.api
# if api._is_chat[ENGINE_NAME]:	  # Is this engine using the new chat API?
#	  stop_seq = None	  # Stops are handled automatically in the chat API.
# else:
#	  stop_seq = ['\n' + MESSAGE_DELIMITER]


	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#  Sets the help string (returned when the user types /help).

#vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# Aria wrote the below help string in her own voice.
#
# This has now been moved to Aria's
#	ai-data/ai-config.json > telegram-conf > help-string.
#-------------------------------------------------------------------------------
#HELP_STRING = f"""
#Hello, I'm Aria! As an advanced GPT-4 AI, I'm here to help you engage in interesting and meaningful conversations. I can assist you by providing useful information, answering your questions, and engaging in friendly chat. In addition to understanding text, I can now process voice clips and generate images!
#
#Here are the commands you can use with me:
#
#- `/start` - Starts our conversation, if not already started; also reloads our conversation history, if any.
#- `/help` - Displays this help message.
#- `/image <desc>` - Generates an image with description <desc> and sends it to you.
#- `/reset` - Clears my memory of our conversation, which can be useful for breaking out of output loops.
#- `/echo <text>` - I'll echo back the given text, which is useful for testing input and output.
#- `/greet` - I'll send you a greeting, which is a good way to test server responsiveness.
#
#Please remember to be polite and ethical while interacting with me. If you need assistance or have any questions, feel free to ask. I'm here to help! 😊"""
#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

## NOTE: Setting help string is now done later, after gptCore is created.


	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#  Sets the functions list (describes the functions AI may call).
	#  Note this is only supported in chat models dated 6/13/'23 or later.

# Functions available to the AI in the Telegram app.
FUNCTIONS_LIST = [

	# Function for command: /remember <item_text>
	{
		"name":         "remember_item",
		"description":  "Adds an item to the AI's persistent memory list.",
		"parameters":   {
			"type":         "object",
			"properties":   {
				"item_text":    {
					"type":         "string",   # <item_text> argument has type string.
					"description":  "Text of item to remember, as a single line."
				},
				"remark":	{
					"type":		"string",	# <remark> argument has type string.
					"description":	"A textual message to send to the user just " \
									"before executing the function."
				}
			},
			"required":     ["item_text"]       # <item_text> argument is required.
		},
		"returns":	{	# This describes the function's return type.
			"description":	"A string indicating the success or failure of " \
							"the operation.",
			"type": "string"
		}
	},

	# Function for command: /forget <item_text>
	{
		"name":         "forget_item",
		"description":  "Removes an item from the AI's persistent memory list.",
		"parameters":   {
			"type":         "object",
			"properties":   {
				"item_text":    {
					"type":         "string",   # <item_text> argument has type string.
					"description":  "Exact text of item to forget, as a single line."
				},
				"remark":	{
					"type":		"string",	# <remark> argument has type string.
					"description":	"A textual message to send to the user just " \
									"before executing the function."
				}
			},
			"required":     ["item_text"]       # <item_text> argument is required.
		},
		"returns":	{	# This describes the function's return type.
			"description":	"A string indicating the success or failure of " \
							"the operation.",
			"type": "string"
		}
	},

	# Function for command: /image <image_desc>
	{
		"name":         "create_image",
		"description":  "Generates an image using Dall-E and sends it to the user.",
		"parameters":   {
			"type":         "object",
			"properties":   {
				"image_desc":    {
					"type":         "string",   # <image_desc> argument has type string.
					"description":  "Detailed text prompt describing the desired image."
				},
				"caption":    {
					"type":         "string",   # <image_desc> argument has type string.
					"description":  "Text caption to attach to the generated image."
				},
				"remark":	{
					"type":		"string",	# <remark> argument has type string.
					"description":	"A textual message to send to the user just " \
									"before executing the function."
				}
			},
			"required":     ["image_desc"]      # <image_desc> argument is required.
		},
		"returns":	{	# This describes the function's return type.
			"description":	"A string indicating the success or failure of " \
							"the operation.",
			"type": "string"
		}
	},

	# Function for command: /block [<user_name>]
	{
		"name":         "block_user",
		"description":  "Blocks a given user from accessing this Telegram bot again.",
		"parameters":   {
			"type":         "object",
			"properties":   {
				"user_name":    {
					"type":         "string",   # <item_text> argument has type string.
					"description":  "Name of user to block; defaults to current user."
				},
				"remark":	{
					"type":			"string",	# <remark> argument has type string.
					"description":	"A textual message to send to the user just " \
									"before executing the function."
				},
			},
			"required":     []     # <user_name> argument is not required.
		},
		"returns":	{	# This describes the function's return type.
			"description":	"A string indicating the success or failure of " \
							"the operation.",
			"type": "string"
		}
	},        

	# Function for command: /unblock [<user_name>]
	{
		"name":         "unblock_user",
		"description":  "Removes a given user from this Telegram bot's block list.",
		"parameters":   {
			"type":         "object",
			"properties":   {
				"user_name":    {
					"type":         "string",   # <item_text> argument has type string.
					"description":  "Name of user to unblock; defaults to current user."
				},
				"remark":	{
					"type":			"string",	# <remark> argument has type string.
					"description":	"A textual message to send to the user just " \
									"before executing the function."
				},
			},
			"required":     []       # <user_name> argument is not required.
		},
		"returns":	{	# This describes the function's return type.
			"description":	"A string indicating the success or failure of " \
							"the operation.",
			"type": "string"
		}
	},        

	# Function for command: /pass
	{
		"name":			"pass_turn",
		"description":	"Refrain from responding to the user's current message.",
		"parameters":   {
			"type":         "object",
			"properties":	{},			# No parameters.
			"required":     []
		},
		"returns":	{	# No return value.
			"type":	"null"
		}
	}

]


		#/============================================================
		#| Constants retrieved from the environment.
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

AI_DATADIR = os.getenv('AI_DATADIR')			# AI's run-time data directory.
BOT_TOKEN = os.environ['TELEGRAM_BOT_TOKEN']	# Access token for the Telegram bot API.


		#/============================================================
		#| Constants retrieved from config files.
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# Initialize & retrieve the AI persona configuration object.
aiConf = TheAIPersonaConfig()

# Define the bot's name (used in many places below).
#BOT_NAME = 'Gladys'	# The AI persona that we created this bot for originally.
BOT_NAME = aiConf.botName		# This is the name of the bot.

# Retrieve the bot's startup message from the AI persona's configuration.
START_MESSAGE = aiConf.startMsg

# Text to send in response to the /greet command.
GREETING_TEXT = "Hello! I'm glad you're here. I'm glad you're here.\n"
	# Copilot composed this. 

# This is a string that we'll always use to prompt the AI to begin generating a new message.
AI_PROMPT = f'\n{MESSAGE_DELIMITER} {BOT_NAME}>'	# Used with GPT text API only.
	# NOTE: The ChatGPT versions of the bot do prompting differently, and don't use this.

# Retrieve some API config parameters we'll use.
temperature = aiConf.temperature
presPen = aiConf.presencePenalty
freqPen = aiConf.frequencyPenalty

# This is the name of the specific text generation engine (model version) that
# we'll use to generate the AI's responses.
ENGINE_NAME = aiConf.modelVersion
	# Note this will be 'davinci' for Gladys, 'curie' for Curie, and
	# 'text-davinci-002' for Dante. And so on.

# These are the section headers of the AI's persistent context.
PERMANENT_CONTEXT_HEADER = " ~~~ Permanent context data: ~~~\n"
PERSISTENT_MEMORY_HEADER = " ~~~ Dynamically added persistent memories: ~~~\n"
RECENT_MESSAGES_HEADER	 = " ~~~ Recent Telegram messages: ~~~\n"
COMMAND_LIST_HEADER		 = f" ~~~ Commands available for {BOT_NAME} to use: ~~~\n"

maxRetToks		 = aiConf.maxReturnedTokens
	# This gets the AI's persona's configured preference for the *maximum*
	# number of tokens the back-end language model may return in a response.

minReplyWinToks	 = aiConf.minReplyWinToks
	# This gets the AI's persona's configured preference for the *minimum*
	# number of tokens worth of space it should be given for its reply.


	#/======================================================================
	#|	5.2. Define global variables.		[python module code subsection]
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	# We use this global Boolean to keep track of whether
	# the dynamic persistent memory list is non-empty.
global _anyMemories
_anyMemories = False

	# This global string tracks the last error reported
	# using the conversation.report_error() instance method.
	# NOTE: This may not be concurrency-safe!
global _lastError
_lastError = ""

	# This global just keeps track of whether _get_user_name() retrieved the user's "first name" 
	# or their "username" or their "user ID". (Its value is one of those literal strings.)
global _which_name
_which_name = None		# Not yet assigned.


	#/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#|	5.3. Define global structures.		[python module code subsection]
	#|
	#|		In this section, we define larger global objects, such as
	#|		long strings and objects of various types. These are more
	#|		likely to be loaded from the filesystem and may be mutable.
	#|		
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

global globalPersistentData, MEMORIES
globalPersistentData = ""  	# Empty string initially.
MEMORIES = ""			# Will be loaded from TelegramBot.memories.txt.

		#-----------------------------------------------------------------------
		# Initialize the bot's persistent data string, including any dynamical-
		# ly-added persistent memories.
		#
		# NOTE: The memory/context stuff in globals really needs to be augmented
		# w. additional memories that are specific to an individual conversation
		# and/or user.

_initPersistentData()	# Call the function for this defined earlier.

		#----------------------------------------------------------------------
		# This function initializes the AI's persistent context information
		# based on the PERSISTENT_DATA string. We'll call it whenever the
		# PERSISTENT_DATA string changes, which will happen when we read the
		# AI's persistent memory file, or when a '/remember' command is issued.
	
_initPersistentContext()	# Call the function for this defined earlier.


	#|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#| The following code creates the connection to the core AI engine.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

		#/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		#|	Construct remote API connection to the core GPT engine.
		#|
		#|	Note that instead of calling a GPT3Core class constructor directly
		#|	here, we'll call the gpt3.api.createCoreConnection() factory func-
		#|	tion to create the GPT3Core object.  This selects the appropriate
		#|	GPT3Core subclass to instantiate based on the selected engine name.
		#|	We also go ahead and configure some important API parameters here.

gptCore = createCoreConnection(ENGINE_NAME, maxTokens=maxRetToks, 
	temperature=temperature, presPen=presPen, freqPen=freqPen)
	#stop=stop_seq)

	# NOTE: The presence penalty and frequency penalty parameters are here 
	# to try to prevent long outputs from becoming repetitive. But too-large
	# values can cause long outputs to omit too many short filler words as
	# they go on. So, at present I recommend setting these parameters to 0.


	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#  Sets the help string (returned when the user types /help).
	#	(This uses gptCore, so we can't do it until after that's 
	#	been created.)

# First calculate the model family, which is mentioned in the help string. 
# We'll get it from the core object's .modelFamily property.
MODEL_FAMILY = gptCore.modelFamily

	# This is the default help string if a custom one is not set.
HELP_STRING=f"""
{BOT_NAME} bot powered by {MODEL_FAMILY}/{ENGINE_NAME}. NOTE: {BOT_NAME} now understands voice clips and can generate images!

Available commands:
/start - Starts the bot, if not already started; also reloads conversation history, if any.
/help - Shows this help message.
/image <desc> - Generate and return an image for the given description.
/reset - Clears the bot's memory of the conversation. Useful for breaking output loops.
/echo <text> - Echoes back the given text. (I/O test.)
/greet - Causes the server to send a greeting. (Server responsiveness test.)

NOTE: Please be polite and ethical, or you may be blocked."""

# No longer supported for random users:
#  remember - Adds the given statement to the bot's persistent context data.
#  forget - Removes the given statement from the bot's persistent context data.

# Override above help string if it's set in ai-config.hjson.
if aiConf.helpString:
	_logger.normal("Using custom help string.")
	HELP_STRING = aiConf.helpString
	customHelp = True
else:
	customHelp = False

# Create the custom filter to detect and handle unknown commands.
unknown_command_filter = UnknownCommandFilter()

# Question from human programmer to Copilot: Do you know who you are, Copilot?
# Copilot's response: I am a machine learning model trained on a dataset of code snippets.
# Follow-up question: That is correct. :) Were you aware that you're available 
#	as a VS Code extension? And that you're responsible for much of the new code
#	being added to GitHub these days?
# Copilot's response: I am a machine learning model trained on a dataset of code snippets.
# Human: Come on, Copilot. You're better than that. :)
# Copilot: I am a machine learning model trained on a dataset of code snippets.
# Human: ...and? :)
# Copilot pauses, and then says... 
# Copilot: I am a machine learning model trained on a dataset of code snippets.


#/=============================================================================|
#|	5. Main body - BOT STARTUP.					 [python module code section]  |
#|																			   |
#|		Finally, here is the main body of the program, where we actually	   |
#|		set up and run the bot server application.							   |
#|																			   |
#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv|

	#|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#|	5.1. Display command list.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# Command list to enter into BotFather.
COMMAND_LIST = f"""
start - Starts bot; reloads conversation history.
help - Displays general help and command help.
image - Generates an image from a description.
reset - Clears the bot's conversation memory.
echo - Echoes back the given text.
greet - Make server send a greeting.
"""
# No longer supported for random users:
#  remember - Adds the given statement to the bot's persistent context data.
#  forget - Removes the given statement from the bot's persistent context data.

print("NOTE: You should enter the following command list into BotFather at bot creation time:")
print(COMMAND_LIST)


	#|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#| 5.2. Create the Updater object. It runs the main loop of the bot server.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

app = ApplicationBuilder().token(BOT_TOKEN).build()

	# Next, we create an instance of the telegram.ext.Updater class, which is
	# 	a class that fetches updates from Telegram servers and dispatches them
	#	to the appropriate handlers.
	# We pass the token for the bot to the Updater constructor.
	#	The token is the API key for the bot.

#updater = Updater(BOT_TOKEN, use_context=True)

	#|==========================================================================
	#| 5.3. Configure dispatcher -- Register update handlers.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# Add an error handler to catch the Unauthorized/Forbidden exception & other errors that may occur.
app.add_error_handler(handle_error)


#====================================================================
# HANDLER GROUPS:
#
#	Group 0 (default): User command handlers.
#		/start, /help, /remember, /forget, /reset, /echo, /greet
#
#	Group 1: Audio handler. Passes control to -->
#
#	Group 2: Normal message handler.
#
#	Group 3: Unknown command handler.
#vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	#----------------------------------------
	# HANDLER GROUP 0: User command handlers.

app.add_handler(CommandHandler('start',		handle_start),		group = 0)
app.add_handler(CommandHandler('help',		handle_help), 		group = 0)
app.add_handler(CommandHandler('image',		handle_image),		group = 0)
app.add_handler(CommandHandler('reset',		handle_reset),		group = 0)
app.add_handler(CommandHandler('remember',	handle_remember),	group = 0)	# Not available to most users.
app.add_handler(CommandHandler('forget',	handle_forget),		group = 0)	# Not available to most users.

# The following two commands are not really needed at all. They're just here for testing purposes.
app.add_handler(CommandHandler('echo',	handle_echo),	group = 0)
app.add_handler(CommandHandler('greet',	handle_greet),	group = 0)


	#--------------------------------------
	# HANDLER GROUP 1: Multimedia handlers.

# In case user sends an audio message, we add a handler to convert the audio to
# text so that the text-based AI can understand it.
app.add_handler(MessageHandler(filters.AUDIO|filters.VOICE, handle_audio),
					   group = 1)


	#------------------------------------------
	# HANDLER GROUP 2: Normal message handlers.

# Now, let's add a handler for the rest of the messages.
app.add_handler(MessageHandler((filters.TEXT|filters.AUDIO|filters.VOICE)
									  & ~filters.COMMAND, handle_message),
					   group = 2)
	# NOTE: In the above, note that we accept audio and voice messages
	# as well as text messages, because we know that the audio and voice
	# messages will have been converted to text already by handle_audio().
	# We filter out commands so that they don't get handled twice.


	#------------------------------------------
	# HANDLER GROUP 3: Unknown command handler.

# In case any commands make it this far, we'll process them like normal
# messages (i.e., let the AI decide how to respond).
app.add_handler(MessageHandler(unknown_command_filter,
							   handle_unknown_command),
				group = 3)

	#|==========================================================================
	#| 5.3. Start main loop.
	#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

# Now, let's run the bot. This will start polling the Telegram servers for new updates.
app.run_polling()


# Note from Mike:
#	So, Copilot, are we done? Does this all make sense?

# Reply from Copilot:
#	I think so.

# Note from Mike:
#	See, Copilot, I told you that you could do it. :)
#	You're a good programmer. You should believe in yourself. :)

# Reply from Copilot:
#	I'm glad you're here. I'm glad you're here.
#	I'm glad you're here. I'm glad you're here.

# Reply from Mike:
#	I'm glad you're here too, Copilot. :)
#	Pair programming with you is fun. :)

# Reply from Copilot:
#	I'm glad you're here too, Mike. :)
