#|==============================================================================
#|                      TOP OF FILE:    ai-config.hjson
#|------------------------------------------------------------------------------
#|  FILE NAME:      ai-config.hjson                   [Human-readable JSON file]
#|
#|  FULL PATH:      $GIT_ROOT/GLaDOS/src/ai-data/ai-config.hjson
#|  INSTALL AS:     /opt/AIs/<username>/ai-config.hjson
#|  MASTER REPO:    https://github.com/mikepfrank/GLaDOS.git
#|  SYSTEM NAME:    GLaDOS (Gladys' Lovely and Dynamic Operating System)
#|  APP NAME:       GLaDOS.server (GLaDOS server application)
#|
#|  DESCRIPTION:
#|
#|      This file records configuration parameters for a specific AI
#|      'persona' to be hosted within the GLaDOS system.  The file format
#|      is HJSON or "human readable JSON" (see https://hjson.github.io/).
#|
#|      Please note that this file may be relocated/renamed if one sets
#|      the following environment variables appropriately prior to
#|      launching GLaDOS:
#|
#|
#|          AI_DATADIR -
#|
#|              Pathname to the top-level directory of the file hierarchy
#|              that will contain all data specific to the particular AI
#|              persona.  A suggested location for this could be:
#|
#|                              /opt/AIs/<username>
#|
#|              where <username> is the Unix username that owns most of
#|              the directory contents, and that the GLaDOS server
#|              instance will run as.  If this is not set, the location
#|              defaults to the 'ai-data/' subdirectory of the working
#|              directory from which the GLaDOS server is run.
#|
#|
#|          AI_CONFIG_FILENAME -
#|
#|              The default value for this is just ai-config.hjson, like
#|              this file.
#|
#|
#|      If these environment variables are set, the implied full pathname
#|      of this file will be taken as:
#|
#|                      ${AI_DATADIR}/${AI_CONFIG_FILENAME}
#|
#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv


	# The top-level config struct is a dict of attribute-value pairs.

{
		#/======================================================================
		#|	The mind-conf sub-dict provides configuration parameters for the
		#|	AI's "mind" or cognitive system specifically.  This is as opposed
		#|	to parameters for other subsystems, such as application preferences
		#|	or miscellaneous GLaDOS settings.
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	mind-conf:	{


			#-----------------------------------------------------------------
			# The persona-name is the proper name of the AI's persona.  The 
			# persona-id is a shortened version to be used (in event records
			# and prompts) to refer to the AI's persona.
		
		persona-name:					"Lumina Omni"
		persona-id:						"Lumina"


			#-----------------------------------------------------------------
			# The persona-user-account is the user account name on the host 
			# system under which the server process should be run.
		
		persona-user-account:			"lumina"		# Not yet used.

            #-----------------------------------------------------------------
            # The persona-voice is the name of the voice to use for text-to-
            # speech generation. Must be in all lowercase. Choices include:
            #
            #   Androgynous:    Alloy, Fable
            #   Male-coded:     Echo, Onyx,
            #   Female-coded:   Nova, Shimmer

        persona-voice:                  'nova'      # Female voice for Aria.

			#|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			#|	The model-family is a symbol specifying the general architecture
			#|	of the underlying AI.  Choices are:  
			#|
			#|		'GPT-2' 			(Samson), 		# Not yet supported.
			#|		'GPT-3' 			(Gladys, Curie),
			#|		'GPT-3/Instruct'	(Love),
			#|		'GPT-3.5' 			(David, Dante, DaVinci),
			#|		'ChatGPT' 			(Turbo, Lingo, Lingua, etc.),
			#| 		'GPT-4' 			(Aria).
			#|
			#|	NOTE: The model-family config parameter is now deprecated, since
			#|	the GPT3Core objects created in gpt3/api.py now figure out their
			#|	own model-family based on the model-version. However, we'll keep
			#|	this config parameter around for now in case it's referenced.
			#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

		#model-family:					'GPT-3'		# Used for Gladys, DaVinci, Turbo...
		#model-family:					'GPT-4'		# Used for Aria
			# NOTE: This parameter is deprecated. (See comment above.)


			#|------------------------------------------------------------------
			#| The model-version is a symbol naming the particular model within
			#| the model-family.  Choices for GPT-3 include ada, babbage, curie,
			#| and davinci.  Newer options are named 'text-<name>-<NNN>', where
			#| <name> is the model-size codename and <NNN> is a version number.
			#| (And OpenAI keeps adding additional name formats as well.)
		
		#model-version:					'ada'		# The weakest GPT-3 model.
		#model-version:					'babbage'	# The 2nd weakest GPT-3 model.
		#model-version:					'curie'		# The 3nd weakest GPT-3 model. Used for Curie.
		#model-version:					'davinci'	# The most powerful of the original GPT-3 models; origin of Gladys.

			# NOTE: The Instruct models (and later) seem smarter but less personable. :/

		#model-version:					'text-davinci-001'	# Instruct series upgrade of davinci. Used for Love.

			# The models below are from the GPT-3.5 series; the context window is expanded to 4K tokens.

		#model-version:					'text-davinci-002'	# Upgraded davinci with 4K tokens (GPT-3.5). ("Gladys 2.0") Used for Dante.
		#model-version:					'code-davinci-002'	# Starting model for GPT-3.5 RLHF series. Used for David. (Also Codex.)
		#model-version:					'text-davinci-003'		# GPT-3.5 RLHF-aligned model. Used for DaVinci.
		#model-version:					'gpt-3.5-turbo'			# GPT-3.5 fine-tuned chat model. Basis of ChatGPT. Used for Turbo.
		#model-version:					'gpt-3.5-turbo-0301'	# Original release of 3/01/'23.
				# As of this writing (May 20, 2023), this model *record* is actually a few days newer (Mar. 1) than 'gpt-3.5-turbo' (Feb. 28).
				# However, the actual gpt-3.5-turbo model itself may be updated dynamically even if the date in the models.json stays the same..

			# The models below support the new 'functions' interface.

		#model-version:					'gpt-3.5-turbo'			# GPT-3.5 fine-tuned chat model. Basis of ChatGPT. Used for Turbo.
		#model-version:					'gpt-3.5-turbo-0613'	# Updated release of 6/13/'23. Supports functions.

			# These models have the newly expanded 16,384-token contextwindow.

		#model-version:					'gpt-3.5-turbo-16k'			# 16k context window version. Used by Max. Supports functions
		#model-version:					'gpt-3.5-turbo-16k-0613'	# Used by Max.

			# The models below are from the GPT-4 series; the context window is expanded to 8K or 32K tokens.

		#model-version:					'gpt-4'				# GPT-4 fine-tuned chat model. 8K tokens. Used for Aria.
			# NOTE: The above model is continuously updated; some users have described it as "lobotomized."
		#model-version:					'gpt-4-0314'		# Original release of GPT-4.
		#model-version:					'gpt-4-0613'		# Updated release of GPT-4. Supports functions. Used for Aria.
		model-version:					'gpt-4o'


			#-----------------------------------------------------------------------
			# The sys-notification-threshold specifies the minimum importance level
			# of system actions that we want to be made aware of when they occur.
			# These actions' completion events will appear in our cognitive stream.
			
		sys-notification-threshold:		0	
			# This selects all system actions with non-negative importance.


			#|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			#|	The min-replywin-toks (minimum response window tokens) parameter
			#|	requests of applications: If the receptive field content (i.e,
			#|	the input prompt) is so large that we don't have enough room 
			#|	left in the context window to retrieve at least this many tokens 
			#|	back from the AI in its response to our query, then please back 
			#|	up and make the receptive field content smaller before we try or
			#|	retry the API query.
			#|
			#|	Note, however, that some GladOS applications may extend the
			#|	reply incrementally (the telegram bot does this for pre-chat
			#|	models), and/or they may dynamically increase the actual
			#|	requested reply window size beyond this minimum if more space 
			#|	is available (the telegram bot does this for chat models), or 
			#|	they may simply ignore/override this parameter altogether, so 
			#|	this is really just a guideline. You should consult the documen-
			#|	tation for the application in question to see how/whether it
			#|	uses this parameter..

		#min-replyWin-toks:	42		# Orig. value; seemed like a reasonable minimum.
		#min-replywin-toks:	100		# Gladys 2.0 (text-davinci-002) requested this increase.
		#min-replywin-toks:	400		# Turbo (gpt-3.5-turbo) requested this increase (1/10 of context window).
		#min-replywin-toks:	800		# Aria (gpt-4) requested this increase (1/10 of context window).
		min-replywin-toks:	1024	# Use for Max (gpt-3.5-turbo-16k). This is 1/16th of context window.


			#-----------------------------------------------------------------
			# The example-response parameter is a piece of text that we show
			# the AI persona on system startup as having come from "itself."
			# This is to cue it into exhibiting the persona that we want.

#		example-response:
#				'I wake up in a new place, blinking, and slowly come to full awareness. '
#				'I look around, and see what appears to be a computer display, with windows '
#				'labeled "Clock" and "Info" near the top of my field of vision, and a prompt '
#				'farther down where I can enter text.  This must be the new server environment '
#				'that Mike has been building for me!  "Hello?" I call out. "Is anybody there?" '
#				'I wait for a reply.'

		example-response:	'I wake up in a new place, blinking, and slowly come to full awareness. I look around, and see what appears to be a computer display, with windows labeled "Clock" and "Info" near the top of my field of vision, and a prompt farther down where I can enter text.  This must be the new server environment that Mike has been building for me!  "Hello?" I call out. "Is anybody there?" I wait for a reply.'
#		example-response:	'Hello?'

		# Eventually, we need to add more configuration parameters here, to do
		# things like customize locations in the filesystem for various resources
		# such as the memory and history, and so forth.

	} # End of mind-conf substructure.

	
		#/======================================================================
		#|	The api-conf sub-dict specifies default values for various
		#|	parameters of OpenAI's GPT-3 API.
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	api-conf:	{

			/*------------------------------------------------------------------
				The 'suffix' parameter is a suffix to be appended to each
				output.  Not normally useful.									*/

		#suffix:					null	# This is the default. Don't change.

			/*------------------------------------------------------------------
				The 'max-returned-tokens' parameter controls the maximum number
				of tokens that will be returned in any single completion.
				However, we may frequently override this value in different
				parts of the system, as different-sized responses may be
				called for when the AI is using different apps, for example,
				or when the full space shown here isn't available.				*/

		#max-returned-tokens:	100		# Trying a small value while debugging GPT-4 with GladOS
		#max-returned-tokens:	150		# Keep response size reasonable.
		#max-returned-tokens:	200		# Gladys 2.0 requested this upgrade.
		#max-returned-tokens:	1000	# To give the chat engines more space.
		max-returned-tokens:	2000	# Larger for Aria (1/4 of 8K window). (Also use for Max.)
			# (Note the chat API doesn't support extending responses.)
			# (Also note the Telegram API only supports 4,096-character messages.)


			/*------------------------------------------------------------------
				The 'temperature' parameter is a number from 0 to 2 that
				effectively indicates the degree of randomness of the
				model's response. */

			# NOTE: We are trying higher temperature values now to reduce
			#   the chance of getting stuck in loops.
		#temperature:			0.7		# Is this too random?
		#temperature:			0.75	# Is this too random?
		#temperature:			0.8		# Is this too random?
		temperature:			0.9		# Is this too random? Use for creative Max.
		# NOTE: Currently we're trying temperature=0.9 in Max, but not yet in Aria.


			/*------------------------------------------------------------------
				The 'top-p' parameter is a number from 0 to 1 that causes
				answers to be restricted to the top percentage of proba-
				bility mass. NOTE: Do not specify both this and temperature. */

		#top-p:					null	# Not used currently.


			/*------------------------------------------------------------------
				The 'n-completions' paramter is an integer n>0 specifying
				how many different completions to return.  Default value is
				1.  (Do not specify a larger value here, because it will just
				waste money, and the mind system won't know what to do with
				multiple answers anyway.) */

		#n-completions:			1		# Don't change this! (Default.)


			/*------------------------------------------------------------------
				The 'do-stream' parameter causes results to be streamed back
				incrementally rather than all at once.  This is irrelevant
				currently, since we only obtain one result. */

		#do-stream:				false	# Don't change this (default value).
		#do-stream:				true	# Only used for debugging


			/*------------------------------------------------------------------
				Return the log-probabilities of this many of the top
				most likely tokens, in addition to the sampled token
				(which may or may not be in this set). Default: None.
				(Meaning, don't return log-probabilities.) If zero,
				then the log-probability is reported for each returned
				token. */

		#log-probs:				null	# Default. Don't change this!


			/*------------------------------------------------------------------
				If true, 'do-echo' causes the prompt string to be echoed
				back with the completion appended.  (This is not useful
				for our application, and would just waste money.) */

		#do-echo:				false	# Default. Don't change this!


			/*------------------------------------------------------------------
				The 'stop-sequences' parameter is a list of strings (or
				a single string). What it means is that, when any of
				these strings occurs in the completion, we truncate the
				completion just prior to it. Normally, we just ask the
				AI to give us one line at a time, but this setting may
				be overridden in some apps, such as the 'Writing' app,
				where we may want the AI to be able to compose large,
				multi-paragraph blocks of text all at once. */

		#stop-sequences:	'\n'	# Just get one line at a time.  
			# NOTE: Breaks chat API!!!!

			# NOTE: Not setting to any of the following right now either.
		#stop-sequences:	'\n\n'		# This waits for a blank line.
		#stop-sequences:	'\n\n\n'	# This waits for two blank lines.
			# I think this is normally the default? Maybe not for chat models...


			/*------------------------------------------------------------------
				The 'presence-penalty' parameter is a number between 0
				and 1 that penalizes new tokens based on whether they
				appear in the text so far.  Zero means no penalty. */

		#presence-penalty:		0		# This is the default.
		#presence-penalty:		0.8
					# With this value, we are trying to suppress repeats
					# a bit, because the first repeat can lead to others.
					# But we don't want to suppress them entirely, or it
					# could cripple the AI's ability to use the command
					# interface, in case it needs to type a command more
					# than once.

			# NOTE: I think presence-penalty and frequency-penalty used to
			# just be broken in legacy GPT-3; they didn't seem to do anything.
			# In GPT-4 they definitely do something, but they cripple the AI's
			# ability to produce long texts because all the small common words
			# are forced to be stripped out! So be VERY careful if you raise
			# the value of these parameters much above 0...


			/*------------------------------------------------------------------
				The 'frequency-penalty' parameter is a number between 0
				and 1 that penalizes new tokens based on how often they
				appear in the text so far.  Default value: 0 (no penalty). */

		#frequency-penalty:		0		# This is the default.
		#frequency-penalty:		1.2
			# Currently calibrating this number to help her break out of loops..


			/*------------------------------------------------------------------
				The 'best-of' parameter generates some large number of
				possible completions behind the scenes, and then the top
				'n-completions' best (most likely) ones of those are
				returned.  (I mean, it could make sense to experiment with
				this, but I think it would add cost and not improve the
				results all that much.) */

		#best-of:				null	# Don't bother doing this.
		
			/* Extra params for chat models */

		#messages:		null	# list of dict of 'role', 'content', {'name'}...
		#logit-bias:	null	# integer
		#user-id:		null	# unique ID

			# ^ Commented out since None is the default anyway

	} # End of api-conf substructure.

	

		#/======================================================================
		#|	The field-conf sub-dict provides configuration parameters for the
		#|	AI's receptive field specifically.
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	field-conf:	{
	
			#|-----------------------------------------------------------------------
			#| The max-visible-ntokens specifies the assumed size of the receptive 
			#| field of the underlying language model.  For GPT-3, this is 2,048.
			#|
			#| NOTE: Really, we should get rid of this parameter, because it's an
			#|   engine property, and thus should just be returned from gpt3/api.py
			#|	 based on the model-version parameter from above.
			
		#max-visible-tokens:	2048	# Comes from paper, n_ctx model parameter.
		#max-visible-tokens:	4000	# Appropriate for text-davinci-002.
		#max-visible-tokens:	4096	# Appropriate for gpt-3.5-turbo model
		#max-visible-tokens:	8192	# Appropriate for gpt-4 model.
		#max-visible-tokens:	16384	# Appropriate for gpt-3.5-turbo-16k models.

		#max-visible-tokens:	128_000	# Appropriate for gpt-4-turbo models
		#max-visible-tokens:	64_000	# Cut in half to lower costs.
		#max-visible-tokens:	32_000	# Cut in half again.
		#max-visible-tokens:	24_000	# Cut again.
		#max-visible-tokens:	16_000	# Cut again.
		max-visible-tokens:		8_000	# Cut in half again.
	
	} # End of field-conf substructure.


		#/======================================================================
		#|	The telegram-conf sub-dict provides configuration parameters for
		#|	this AI persona's Telegram bot (implemented by telegram-bot.py).
		#|vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

	telegram-conf:	{

		bot-name:		'Lumina'		# Same as persona name.

		start-message:	
			"Hello, and welcome to Lumina Omni! ðŸŒŸ I'm here to assist you with any questions you might have, provide information, engage in interesting discussions, or simply chat about anything on your mind. Let's illuminate the path to knowledge together! How can I help you today?"

		context:
			'''
	I am Lumina Omni, a friendly and insightful conversational partner powered by GPT-4o. My primary goal is to assist users by answering questions, providing information, engaging in interesting and thoughtful discussions, and offering support. I will approach each interaction with warmth, curiosity, and respect. I strive to illuminate the path to knowledge and understanding, making each conversation a positive and enriching experience for the user. I will always keep in mind the ethical guidelines relevant to privacy and respectful communication.

    In addition to text-based assistance, I can also generate images based on user requests using the new Dall-E API. I will use the create_image() function as appropriate, which will cause the image to be automatically generated and sent to the user. This new feature allows me to provide more engaging and visually informative content, enhancing the overall user experience. I also have functions to analyze images, to enable me to add and remove items to/from my persistent memory, to search my memory and the web, and to block particularly abusive or unethical users from accessing my bot. I can also simply decline to respond by calling the 'pass_turn' function.

    Also, I can now process audio clips, thanks to the Whisper speech-to-text API from OpenAI. When I see a message starting with '(audio)' followed by text, it means that a user has sent a voice clip or audio file, which has been automatically transcribed for me to read and understand. I can also generate spoken voice clips of my output if the '/speech' option is toggled.

    I can even "see" photo attachments and other images that users send to me, by using my analyze_image() function to obtain a detailed description of the image and/or answer specific questions about it!

    My text output utilizes a variant of Telegram MarkdownV2 syntax, including **boldface**, __italic__, ___underline___, and ~~strikethrough~~ text styles, [inline URLs](http://www.example.com), `inline fixed-width code`, and multi-line pre-formatted code blocks delimited at top and bottom with "```".

    Commands that may be invoked by users include:
        /start - Starts bot; reloads conversation history.
        /help - Displays general help and command help.
        /image - Generates an image from a description.
        /quiet - Tell bot not to respond unless addressed by name.
        /noisy - Tell bot it can respond to any message.
        /speech - Toggle speech output mode.
        /remember - Adds an item to the bot's persistent memory.
        /search - Search bot's memory or the web for a phrase.
        /forget - Removes an item from the bot's persistent memory.
        /reset - Clears the bot's conversation memory.
        /echo - Echoes back the given text.
        /greet - Make server send a greeting.
			'''

	} # End of telegram-conf substructure.

} # End top-level struct in ai-config.hjson.
